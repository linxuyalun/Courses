* [Distributed systems for fun and profit](http://book.mixu.net/distsys/index.html)

# Introduction

在本文中，我试图为分布式系统提供更易于理解的介绍。 对我而言，这意味着两件事：介绍需要的关键概念，以便有更好的时间阅读更严肃的文本，并提供一个足够详细的事情的叙述，了解正在发生的事情而不会卡住在细节上。 

在我看来，许多分布式编程都是关于处理以下两个内容：

* information travels at the speed of light
* independent things fail independently*

换句话说，分布式编程的核心是处理距离（呃！）和多台计算机（呃！）。 这些约束定义了可能的系统设计空间，我希望在阅读完之后，你将更好地了解距离，时间和一致性模型如何相互作用。

本文重点介绍了解数据中心中商业系统所需的分布式编程和系统概念。 你将学习许多关键协议和算法（例如，涵盖该学科中许多被引用次数最多的论文），包括一些新的令人兴奋的方法来研究最终的一致性，这些方法还没有进入大学教科书——例如CRDT 和CALM定理。

## Basics

[第一章](#1-distributed-systems-at-a-high-level)通过介绍一些重要的术语和概念，从高层次上介绍了分布式系统。 它涵盖了高级目标，例如可伸缩性，可用性，性能，延迟和容错; 这些是如何难以实现的，以及抽象和模型以及分区和复制如何发挥作用。

## Up and down the level of abstraction

[第二章](#2-up-and-down-the-level-of-abstraction)深入研究了抽象和不可能的结果。 它以Nietzsche引用开始，然后介绍系统模型和在典型系统模型中做出的许多假设。 然后讨论了CAP定理并总结了FLP不可能性结果。 然后转向CAP定理的含义，其中之一是应该探索其他一致性模型。 然后讨论了许多一致性模型.

## Time and order

理解分布式系统的一个重要部分是了解时间和顺序。 如果我们无法理解和模拟时间，我们的系统将会失败。 [第三章](#3-time-and-order)讨论时间和顺序，时钟以及时间，顺序和时钟（如矢量时钟和故障检测器）的各种用途。

## Replication: preventing divergence

第四章介绍了复制问题，以及它可以执行的两种基本方法。 事实证明，大多数相关特征可以通过这种简单的表征来讨论。 然后，从最小容错（2PC）到Paxos讨论了用于维护单拷贝一致性的复制方法。

## Replication: accepting divergence

第五章讨论了弱一致性保证的复制。 它引入了一个基本的协调方案，其中分区副本尝试达成协议。 然后，它讨论了亚马逊的Dynamo作为具有弱一致性保证的系统设计的示例。 最后，讨论了关于无序编程的两个观点：CRDT和CALM定理。

# 1. Distributed systems at a high level

> Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers.

任何计算机系统都需要完成两个基本任务：

* 存储
* 计算

分布式编程是一种艺术，它可以用多台计算机解决单台计算机上的相同问题——通过，这种问题不再适用于单台计算机。

不是说非要用分布式系统。如果给定无限的资金和无限的研发时间，我们不需要分布式系统。 所有的计算和存储都可以在一个神奇的盒子上完成 ——一个单一的，令人难以置信的快速和令人难以置信的可靠系统。

但是，谁又拥有无限的资源呢。 因此，我们必须在一些现实世界的成本效益曲线上找到合适的位置。 在小规模的时候，升级硬件是一种可行的策略。 但是，随着问题规模的增加，当你想要解决的问题无法通过升级硬件解决或者升级硬件的成本太高的时候——欢迎来到分布式系统的世界。

目前的现实情况是，采用中档商品硬件——维护成本可以通过容错软件来降低。

计算主要受益于高端硬件，它们可以用内部内存访问来代替慢速网络访问。 但是当遇到在节点之间需要大量通信的任务时，高端硬件的性能优势就会受到限制。

![figure 1](http://book.mixu.net/distsys/images/barroso_holzle.png)

如上图所示，假设所有节点都有统一的内存访问模式，高端硬件和商用硬件之间的性能差距随着cluster的增大而减小。

理想情况下，添加新机器将线性地提高系统的性能和容量。 但当然这是不可能的，因为这些单独的计算机会产生一些额外的开销。 需要复制数据，必须协调计算任务等。 这就是研究分布式算法的原因——它们为特定问题提供了有效的解决方案，并提供了可行的指导，正确实施的最低成本是什么，以及什么是不可能的。

本文的重点是分布式编程和系统中一个普通但和商业上息息相关的设置：数据中心。 例如，我不会讨论由异常网络配置或共享内存设置中出现的特殊问题。 此外，重点是探索系统设计空间而不是优化任何特定设计——后者是更专业化的主题。

## What we want to achieve: Scalability and other good things

在我看来，一切都从对处理规模（size）的需要开始。

大多数事情在小规模上都是微不足道的，但是一旦超过一定的规模，数量或其他物理上有限的事情，同样的问题就会变得更加困难。 提起一块巧克力很容易，很难举起一座山。 很容易计算一个房间里有多少人，很难计算一个国家有多少人。

所以一切都从规模开始——可扩展性。可扩展性，简单的说，就是在可扩展系统中，当事情从小变大，事情不应该越来越糟。 这是另一个定义：

>[Scalability](http://en.wikipedia.org/wiki/Scalability): is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth.

什么是增长？ 几乎可以用任何方式衡量增长（人数，用电量等）。但是有三个特别有趣的事情要看：

* 规模可扩展性：添加更多节点应该使系统线性更快; 增长数据集不应该增加延迟
* 地理可扩展性：应该可以使用多个数据中心来减少响应用户查询所需的时间，同时以一种合理的方式处理跨数据中心延迟。
* 管理可扩展性：添加更多节点不应增加系统的管理成本（例如管理员与机器的比率）。

当然，在实际系统中，增长同时发生在多个不同的轴上；每个指标都只捕获了增长的某些方面。

可扩展系统是随着规模的增加而不断满足其用户需求的系统。 有两个特别相关的方面——性能和可用性——可以通过各种方式进行衡量。

### Performance (and latency)

> [Performance](http://en.wikipedia.org/wiki/Computer_performance): is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.

根据具体情况，这可能涉及实现以下一项或多项：

* 对于给定的工作，响应时间短/低延迟
* 高吞吐量（处理工作率）
* 计算资源利用率低

优化任何这些结果都需要权衡。 例如，系统可以通过处理更大批量的工作来实现更高的吞吐量，从而减少操作开销。 但是由于批处理，权衡对于各个工作的响应时间会更长。

低延迟——实现较短的响应时间——是性能中最有趣的方面，因为它与物理（而非金钱）限制有很强的联系。 使用钱去解决延迟比执行其他方面更困难。

延迟有很多非常具体的定义，但是通过词源去理解延迟真的很有意思：

> Latency: The state of being latent; delay, a period between the initiation of something and the occurrence.

那么“latent”是什么意思？

> Latent: From Latin latens, latentis, present participle of lateo ("lie hidden"). Existing or present but concealed or inactive.

这个定义非常酷，因为它突出了延迟实际上是事件发生后变得可见需要的时间。

例如，假设感染了一种将人变成僵尸的空气传播病毒。 潜伏期是被感染之后和变成僵尸之间的时间——这就是延迟：已经发生的事情被隐藏起来的时间。

让我们暂时假设我们的分布式系统只执行一个高级任务：给定一个查询，它会获取系统中的所有数据并计算单个结果。 换句话说，将分布式系统视为数据存储，能够在其当前内容上运行单个确定性计算（函数）：

```
result = query(all data in the system)
```

那么，对于延迟而言重要的不是旧数据的数量，而是新数据在系统中“生效”的速度。 例如，延迟可以这样衡量——how long it takes for a write to become visible to readers。

基于这个定义的另一个关键点是，如果无事发生，就没有“潜伏期”。 数据不变的系统不会（或不应该）存在延迟问题。

在分布式系统中，存在无法克服的最小延迟：光速限制了信息传输的速度，硬件组件每次操作都会产生的最低延迟花费（想想RAM和硬盘驱动器以及CPU）。

最小延迟对查询的影响程度取决于这些查询的性质以及信息需要传输的物理距离。

### Availability (and fault tolerance)

可扩展系统的第二个方面是可用性。

>[Availability](http://en.wikipedia.org/wiki/High_availability): the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable.

分布式系统使我们能够实现在单个系统上难以实现的理想特性。例如，单个机器无法容忍任何故障，因为它要么故障要么正常运行。

分布式系统可以采用一堆不可靠的组件，并在它们之上构建可靠的系统。

没有冗余的系统只能作为其底层组件可用。 使用冗余构建的系统可以容忍部分故障，因此可用性更高。 值得注意的是，“冗余”可能意味着不同的东西，具体取决于所看到的内容——组件，服务器，数据中心等。

在公式上，可用性是：`Availability = uptime / (uptime + downtime)`

从技术角度来看，可用性主要是关于容错。 因为发生故障的概率随着组件的数量的增加而增加，所以系统应该能够进行补偿，以防止随着组件数量的增加而变得不那么可靠。

比如：

| Availability %         | How much downtime is allowed per year? |
| ---------------------- | :------------------------------------- |
| 90% ("one nine")       | More than a month                      |
| 99% ("two nines")      | Less than 4 days                       |
| 99.9% ("three nines")  | Less than 9 hours                      |
| 99.99% ("four nines")  | Less than an hour                      |
| 99.999% ("five nines") | ~ 5 minutes                            |
| 99.9999% ("six nines") | ~ 31 seconds                           |

从某种意义上说，可用性是一个比正常运行时间更广泛的概念，因为服务的可用性也会受到网络中断或拥有该服务的公司的影响（这是一个与容错无关但仍会影响系统可用性的因素）。 但是，如果不了解系统的每个特定方面，我们所能做的最好的是容错设计。

什么是容错？

> Fault tolerance: ability of a system to behave in a well-defined manner once faults occur

容错归结为：定义一个期望的故障，然后设计一个容忍它们的系统或算法。

## What prevents us from achieving good things?

分布式系统受两个物理因素的限制:

* 节点数（随着所需的存储和计算能力增加而增加）
* 节点之间的距离（认为以光速进行信息传播）

在这些限制范围内工作：

* 独立节点数量的增加会增加系统故障的可能性（降低可用性并增加管理成本）
* 独立节点数量的增加可能会增加节点之间通信的需求（随着规模的增加而降低性能）
* 地理距离的增加会增加远程节点之间通信的最小延迟（降低某些操作的性能）

除了这些限制——这是物理限制的结果——剩下就是系统设计选择的世界。

性能和可用性都由系统的外部保证定义。 在较高的层面上，可以将保证视为系统的SLA（service level agreement）：如果我写数据，我可以多快在其他地方访问它？ 写完数据后，我有什么保证这些数据的耐用性？ 如果我要求系统运行计算，它返回结果的速度有多快？ 当组件发生故障或停止运行时，这会对系统产生什么影响？

还有另一个标准，没有明确提及但暗示：可理解性。 当然，没有简单的指标来衡量什么是可理解性。

我有点想在物理限制下加入“可理解性”。 毕竟，对于人来说，这是一个硬件限制，我们很难理解任何涉及比手指更动人的东西。 这是错误和异常之间的区别——错误是不正确的行为，而异常是意外行为。 如果你更聪明，你会发现异常发生。

## Abstractions and models

这就是抽象和模型发挥作用的地方。 通过消除与解决问题无关的现实世界方面，抽象使事情更易于管理。 模型以精确的方式描述分布式系统的关键属性。 我将在下一章讨论多种模型，例如：

- System model (asynchronous / synchronous)
- Failure model (crash-fail, partitions, Byzantine)
- Consistency model (strong, eventual)

良好的抽象使得使用系统更容易理解，同时捕获与特定目的相关的因素。

在存在许多节点的现实与我们对“像单个系统一样工作”的系统的需求之间存在着紧绷关系。 通常，最熟悉的模型（例如，在分布式系统上实现共享内存抽象）太昂贵了。

制定较弱保证的制度具有更大的行动自由，因此可能具有更高的性能，但也可能难以推理。 人们更擅长推理像单个系统一样工作的系统，而不是节点集合。

人们通常可以通过暴露有关系统内部的更多细节来获得性能。 例如，在列式存储中，用户可以（在某种程度上）推断系统内键值对的位置，从而做出影响典型查询性能的决策。 隐藏这些细节的系统更容易理解（因为它们更像是单个单元，需要考虑更少的细节），而暴露更多真实细节的系统可能有更好的性能（因为它们更接近现实）。

几种类型的故障使得编写像单个系统一样的分布式系统变得十分困难。 网络延迟和网络分区（例如某些节点之间的总网络故障）意味着系统有时需要做出艰难的选择，以确定是否更好地保持可用但丢失一些无法实施的关键保证，或者在发生这些类型的故障时保证安全并拒绝客户端。

最后，理想的系统满足程序员的需求（干净的语义）和业务需求（可用性/一致性/延迟）。

## Design techniques: partition and replicate

数据集在多个节点之间分配的方式非常重要。为了进行任何计算，我们需要定位数据然后对其进行操作。

有两种基本技术可以应用于数据集。 它可以分割为多个节点（分区）以允许更多并行处理。 它还可以复制或缓存在不同的节点上，以减少客户端和服务器之间的距离，并提高容错能力（复制）。

> Divide and conquer - I mean, partition and replicate.

下图说明了这两者之间的区别：分区数据（下面的A和B）被分成独立的集合，而复制的数据（下面的C）被复制到多个位置。

![figure2](http://book.mixu.net/distsys/images/part-repl.png)

这是解决分布式计算中任何问题的组合拳。 当然，诀窍在于为具体实施选择正确的技术; 有许多算法实现复制和分区，每个算法都有不同的限制和优点，需要根据设计目标进行评估。

### Partitioning

分区是将数据集划分为更小的不同独立集合;这用于减少数据集增长的影响，因为每个分区都是数据的子集。

* 分区通过限制要检查的数据量并通过在同一分区中分配相关数据来提高性能；
* 分区通过允许分区独立失效来提高可用性，从而增加在牺牲可用性之前失效的节点数。

分区也是特定于应用程序的，因此在不了解具体细节的情况下很难说清楚。这就是为什么大多数文章重点放在复制，包括本文。

分区主要是根据认为的主要访问模式来定义分区，并处理来自独立分区的限制（例如跨分区的低效访问，不同的增长率等）。

### Replication

复制是在多台机器上复制相同的数据;这允许更多服务器参与计算。

> To replication! The cause of, and solution to all of life's problems.

复制——复制或再现某些东西，是分布式系统抵御延迟的主要方式。

* 复制通过用额外的计算能力和带宽在新的数据副本来提高性能；
* 复制通过创建数据的其他副本来提高可用性，从而增加了在牺牲可用性之前失败的节点数。

复制是关于提供额外的带宽，以及缓存重要的内容。它还涉及根据某种一致性模型以某种方式保持一致性。

复制允许我们实现可伸缩性，性能和容错。 害怕可用性或性能降低？ 复制数据以避免瓶颈或单点故障。 计算慢？ 在多个系统上复制计算。 慢I/O？ 将数据复制到本地缓存以减少延迟，或将数据复制到多台计算机上以提高吞吐量。

复制也是许多问题的根源，因为必须在多台机器上保持同步的数据的独立副本——这意味着确保复制遵循一致性模型。

一致性模型的选择至关重要：良好的一致性模型为程序员提供了干净的语义（换句话说，它保证的属性易于推理）并满足业务/设计目标，如高可用性或强一致性。

只有一个用于复制的一致性模型——强一致性——允许你进行编程，就好像未复制基础数据一样。其他一致性模型将复制的一些内部暴露给程序员。 但是，较弱的一致性模型可以提供较低的延迟和较高的可用性 - 并且不一定难以理解，只是不同。

## Further reading

- [The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines](http://www.morganclaypool.com/doi/pdf/10.2200/s00193ed1v01y200905cac006) - Barroso & Hölzle, 2008
- [Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing)
- [Notes on Distributed Systems for Young Bloods](http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/) - Hodges, 2013

[返回Introduction](#introduction)

# 2. Up and down the level of abstraction

在本章中，我们将在抽象层次上下移动，查看一些不可能的结果（CAP和FLP），然后为了性能而向下移动。

只要写过任何编程，那么抽象级别的概念就很熟悉。 我们始终在某种抽象级别工作，通过某些API与较低级别层接口，并可能为用户提供一些更高级别的API或用户界面。 计算机网络的七层OSI模型就是一个很好的例子。

我断言，分布式编程在很大程度上处理了分布的后果。 也就是说，我们对“像单一系统一样工作”的分布式系统的需求和现实情况多节点的分布式系统之间存在一个紧绷关系。 这意味着要找到一个好的抽象，以平衡可理解性和性能。

存在许多节点的现实与我们对“像单一系统一样工作”的系统的需求之间存在着紧张关系。 这意味着要找到一个好的抽象，以平衡可能与可理解和高效的东西。

当说X比Y更抽象时，到底意味着什么？ 首先，X不会引入任何新的或与Y基本不同的东西。事实上，X可能会删除Y的某些方面或以一种使它们更易于管理的方式呈现它们。 其次，假设从Y中移除的X对于手头的事情并不重要，X在某种意义上比Y更容易掌握。

正如[Nietzsche](http://oregonstate.edu/instruct/phl201/modules/Philosophers/Nietzsche/Truth_and_Lie_in_an_Extra-Moral_Sense.htm)写得那样：

>Every concept originates through our equating what is unequal. No leaf ever wholly equals another, and the concept "leaf" is formed through an arbitrary abstraction from these individual differences, through forgetting the distinctions; and now it gives rise to the idea that in nature there might be something besides the leaves which would be "leaf" - some kind of original form after which all leaves have been woven, marked, copied, colored, curled, and painted, but by unskilled hands, so that no copy turned out to be a correct, reliable, and faithful image of the original form.
>
>每个概念都源于我们将不平等的东西等同起来。 没有叶子完全等于另一个叶子，“叶子”的概念是通过对这些个体差异的任意抽象，通过忘记区别而形成的。现在，产生了这样一种想法：在自然界中，除了叶子之外，可能还有其他什么东西是“叶子”——一种最初的形式，在这种形式之后，所有的叶子都被编织、标记、复制、着色、卷曲和上色，但都是用不熟练的手完成的，所以没有一个复制品是原来形式的正确、可靠和忠实的形象。

从根本上说，抽象是假的。 每种情况都是独特的，每个节点都是如此。 但是抽象使得世界变得易于管理：更简单的问题陈述——没有现实世界——更易于分析，只要我们不忽视任何必要的东西，解决方案就可以广泛应用。

实际上，如果我们保留的东西是必不可少的，那么我们可以得出的结果将是广泛适用的。 这就是之前提及的不可能的结果如此重要的原因：它们采用最简单的问题表达方式，并证明在某些约束或假设中无法解决。

所有的抽象都忽略了一些有利于将现实中独一无二的事物等同起来的东西。 诀窍是摆脱一切不重要的东西。 你怎么知道什么是必要的？ 先验。

每次我们从系统规范中排除系统的某些方面时，我们都会冒险引入错误源和/或性能问题。 这就是为什么有时我们需要走向另一个方向，并有选择地介绍真实硬件和现实世界问题的某些方面。 重新引入一些特定的硬件特性（例如物理顺序性）或其他物理特性以获得性能足够好的系统。

考虑到这一点，当我们仍在处理一些仍然可以识别为分布式系统的东西时，我们可以保留的最少的“现实量”是多少？系统模型是我们认为重要的特性的规范；指定了一个特性之后，我们就可以看看一些不可能的结果和挑战。

## A system model

分布式系统的关键属性是分布。更具体地说，分布式系统中的程序：

* 在独立节点上并发运行；
* 网络连接可能引入不确定性和消息丢失的；
* 没有共享内存或共享时钟。

这里有很多含义：

* 每个节点同时执行一个程序；
* 知识是本地的：节点只能快速访问其本地状态，任何有关全局状态的信息都可能超时；
* 节点可能会故障并独立地从故障中恢复；
* 消息可能会延迟或丢失（这与节点故障无关——区分网络故障和节点故障并不容易）；
* 时钟不跨节点同步（本地时间戳与全局顺序不对应，无法轻易观察到）。

系统模型列举了与特定系统设计相关的许多假设。

> System model: a set of assumptions about the environment and facilities on which a distributed system is implemented

系统模型对环境和设施的假设各不相同。这些假设包括：

* 节点具有哪些功能以及它们可能如何失效；
* 通信链路如何运作以及它们如何失效；
* 整个系统的属性，例如关于时间和顺序的假设。

一个鲁棒性最强的系统模型是做出最弱假设的模型：为这样的系统编写的任何算法 能容忍不同的环境，因为它做出非常少且非常弱的假设。

换言之，我们可以通过作出强有力的假设来创建一个易于推理的系统模型。例如，假设节点没有失败意味着我们的算法不需要处理节点失败。显然，这样的系统模型是不现实的，因此很难应用到实践中。

让我们更详细地看一下节点，链接以及时间和顺序的属性。

### Nodes in our system model

节点充当计算和存储的主机。它们有：

* 执行程序的能力
* 能够将数据存储到易失性存储器（可能在发生故障时丢失）并进入稳定状态（可在故障后读取）
* 时钟（可能会或可能不会被认为是准确的）

节点执行确定性算法：本地计算，计算后的本地状态以及发送的消息由接收到的消息和接收消息时的本地状态唯一确定。

有许多可能的失效模型描述了节点失效的方式。在实践中，大多数系统都假设一个崩溃-恢复的失败模型：即节点只能通过崩溃来失败，并且在稍后的某个时刻崩溃后可以（可能）恢复。

另一种选择是假设节点可以通过任意方式的错误行为而失败。这被称为拜占庭容错。拜占庭式错误在现实世界的商业系统中很少被处理，因为对任意错误有弹性的算法运行起来更昂贵，实现起来也更复杂。我不会在这里讨论它们。

### Communication links in our system model

通信链路将各个节点相互连接，并允许消息以任意方向发送。许多讨论分布式算法的书籍假定每对节点之间都有单独的链接，这些链接为消息提供FIFO顺序，它们只能传递已发送的消息，并且发送的消息可能会丢失。

一些算法假定网络是可靠的：消息不会丢失，也不会无限期延迟。对于某些实际设置来说，这可能是一个合理的假设，但一般来说，最好考虑网络不可靠，并且会受到消息丢失和延迟的影响。

当网络出现故障，而节点本身仍在运行时，就会出现网络分区。出现这种情况时，消息可能会丢失或延迟，直到网络分区修复。分区节点可以被一些客户机访问，因此必须区别于崩溃的节点。下图说明了节点故障与网络分区的关系：

![](http://book.mixu.net/distsys/images/system-of-2.png)

很少对通信链路做进一步的假设。我们可以假设链接只在一个方向上工作，或者我们可以为不同的链接引入不同的通信成本（例如，由于物理距离造成的延迟）。但是，在商业环境中，除了长距离链路（广域网延迟）之外，这些问题很少被关注，因此我不会在这里讨论它们；更详细的成本和拓扑模型允许以复杂性为代价进行更好的优化。

### Timing / ordering assumptions

物理分布的结果之一是每个节点以独特的方式体验世界。这是不可避免的，因为信息只能以光速传播。如果节点之间的距离不同，那么从一个节点发送到另一个节点的任何消息将在不同的时间到达，并且可能在其他节点以不同的顺序到达。

时间假设是一种方便的简写，用于捕捉关于我们将这一现实考虑在内的程度的假设。两个主要的替代方案是：

> Synchronous system model: Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock
>
> Asynchronous system model: No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist

同步系统模型对时间和顺序施加了许多约束。它基本上假定节点具有相同的经历：发送的消息总是在特定的最大传输延迟内接收，并且进程在锁定步骤中执行。这很方便，因为它允许您作为系统设计者对时间和顺序进行假设，而异步系统模型则不是如此。

异步性是一种非假设：它只是假设你不能依赖于时间（或“时间传感器”）。

同步系统模型中的问题更容易解决，因为关于执行速度、最大消息传输延迟和时钟精度的假设都有助于解决问题，因为您可以根据这些假设进行推断，并通过假设不发生不方便的故障场景来排除它们。

当然，假设同步系统模型不太现实。现实世界中的网络容易出现故障，并且消息延迟没有严格的限制。现实世界中的系统充其量只是部分同步的：它们可能偶尔会正常工作并提供一些上限，但有时消息会无限期延迟，时钟也会不同步。我不会在这里真正讨论同步系统的算法，但您可能会在许多其他介绍性书籍中遇到它们，因为它们在分析上更容易（但不现实）。

### The consensus problem

在本文的其余部分，我们将改变系统模型的参数。接下来，我们将看看如何改变两个系统属性：

* 网络分区是否包含在故障模型中，以及
* 同步与异步时间假设

通过讨论两个不可能的结果（FLP和CAP）来影响系统设计选择。

当然，为了进行讨论，我们还需要引入一个问题来解决。我要讨论的问题是共识问题。

如果几个计算机（或节点）都同意一些价值，那么它们就会达成共识。更正式地说：

* Agreement: Every correct process must agree on the same value.
* Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.
* Termination: All processes eventually reach a decision.
* Validity: If all correct processes propose the same value V, then all correct processes decide V.

共识问题是许多商业分布式系统的核心。毕竟，我们希望一个分布式系统的可靠性和性能不必处理分布式带来的一些不好的后果（例如节点之间的分歧），解决共识问题可以解决几个相关的、更高级的问题，例如原子广播和原子提交。

### Two impossibility results

第一个不可能结果称为FLP不可能结果，主要和设计分布式算法的人特别相关。第二个——CAP定理——是一个与实践者更相关的相关结果——那些需要在不同系统设计之间进行选择，但不直接关注算法设计的人。

## The FLP impossibility result

虽然在学术界被认为是更重要的，但我只会简单地总结一下FLP不可能的结果。FLP不可能的结果（以作者Fischer、Lynch和Patterson的名字命名）检验了异步系统模型下的共识问题（从技术上讲，是共识问题的一种非常弱的形式）。假设节点只能通过崩溃而失败；网络可靠；异步系统模型的典型时间假设成立：例如，消息延迟没有限制。

在这些假设下，FLP结果表明，“在容易发生故障的异步系统中，不存在共识问题的（确定性）算法，即使消息永远不会丢失，至多一个进程可能会失败，并且只能通过崩溃（停止执行）来失败。”

这个结果意味着，**在一个极小的系统模型下，没有办法以一种永远不能延迟的方式来解决共识问题**。论证是，如果存在这样的算法，那么可以设计一种算法的执行，在这种算法中，通过延迟消息传递（异步系统模型中允许），它将在任意时间内保持不确定（“二价”）。因此，这种算法不可能存在。

这种不可能的结果很重要，因为它强调了假设异步系统模型会导致一种权衡：解决共识问题的算法必须在消息传递边界的保证不成立时放弃安全性或活跃性。

这种见解与设计算法的人尤其相关，因为它对异步系统模型中我们知道可以解决的问题施加了一个硬约束。CAP定理是一个与实践者更相关的相关定理：它做出了稍微不同的假设（网络故障而不是节点故障），并对实践者在系统设计之间进行选择具有更明确的含义。

## The CAP theorem

CAP定理最初是由计算机科学家Eric Brewer提出的猜想。 在系统设计的保证中考虑权衡是一种流行且相当有用的方法。该定理说明了这三个属性：

* 一致性：所有节点同时看到相同的数据；
* 可用性：节点故障不会阻止幸存者继续运行；
* 分区容差：尽管由于网络和/或节点故障导致消息丢失，系统仍继续运行。

只有两个可以同时满足。我们甚至可以将它绘制成一个漂亮的图表，从三个中选择两个属性为我们提供了三种类型的系统，它们对应于不同的交叉点：

![](http://book.mixu.net/distsys/images/CAP.png)

注意，该定理表明中间件（具有所有三个属性）是不可实现的。然后我们得到三种不同的系统类型：

* CA（一致性+可用性）。示例包括完全严格的仲裁协议，例如两阶段提交；
* CP（一致性+分区容差）。示例包括多数分区协议，例如Paxos；
* AP（可用性+分区容差）。示例包括使用冲突解决的协议，例如Dynamo。

CA和CP系统设计都提供了相同的一致性模型：强一致性。唯一的区别是，CA系统不能容忍任何节点故障；在非拜占庭故障模型中，给定2f+1节点，CP系统可以容忍最多f个的故障（换句话说，只要大多数f+1保持不变，它可以容忍少数f个节点的故障）。原因很简单：

* CA系统不能区分节点故障和网络故障，因此为了避免引起分歧（多个副本），CA系统必须在所有地方都停止写操作。它无法判断远程节点是否关闭，或者只是网络连接是否关闭：所以唯一安全的事情就是停止接受写操作；
* CP系统通过强制分区两侧的非对称行为来防止分歧（例如保持单个拷贝的一致性）。它只保留大多数分区，并要求少数分区变为不可用（例如停止接受写入），从而保持一定程度的可用性（多数分区），并且仍然确保单个副本的一致性。

当我讨论paxos时，我将在关于复制的那一章中更详细地讨论这个问题。重要的是，CP系统将网络分区合并到其故障模型中，并使用诸如paxos、raft或viewstamped复制之类的算法区分大多数分区和少数分区。CA系统不支持分区，而且在历史上更为常见：它们通常使用两阶段提交算法，并且在传统的分布式关系数据库中很常见。

假设发生了分区，该定理简化为可用性和一致性之间的二元选择。

![](http://book.mixu.net/distsys/images/CAP_choice.png)

我认为应该从CAP定理中得出四个结论：

首先，**在早期的分布式关系数据库系统中使用的许多系统设计没有考虑分区容差**（例如，它们是CA设计）。分区容差是现代系统的一个重要属性，因为如果系统是地理分布的（和许多大型系统一样），那么网络分区的可能性就大得多。

其次，**在网络分区期间，强一致性和高可用性之间存在紧绷关系**。 CAP定理说明了强保证和分布式计算之间的权衡。

在某种意义上，承诺由不可预知网络连接的独立节点组成的分布式系统“以与非分布式系统不可区分的方式运行”是非常疯狂的。

强一致性保证要求我们在分区期间放弃可用性。这是因为在继续接受分区两侧的写操作的同时，不能防止两个无法相互通信的副本之间的分歧。

我们如何解决这个问题？通过加强假设（假设没有分区）或削弱担保。一致性可以与可用性（以及离线可访问性和低延迟的相关功能）进行权衡。如果“一致性”被定义为小于“所有节点同时看到相同的数据”，那么我们可以同时拥有可用性和一些（较弱的）一致性保证。

第三，**在正常操作中强一致性和性能之间存在紧绷关系**。

强一致性/单拷贝一致性要求节点在每个操作上进行通信并达成一致。这导致正常操作期间的高延迟。

如果可以使用传统一致性模型以外的一致性模型（允许副本延迟或分散的一致性模型），那么可以减少正常操作期间的延迟，并在存在分区的情况下保持可用性。

当涉及的消息更少且节点更少时，操作可以更快地完成。但实现这一目标的唯一方法是放宽保证：让一些节点的联系频率降低，这意味着节点可以包含旧数据。

这也使异常发生成为可能——不再保证获得最新价值。根据所提供的保证类型，可能会读取比预期更早的值，甚至会丢失一些更新。

第四，**如果我们不想在网络分区期间放弃可用性，那么我们需要探索除了强一致性之外的一致性模型是否可用于我们的目的**。

例如，即使将用户数据地理复制到多个数据中心，并且这两个数据中心之间的链接暂时不正常，在许多情况下，我们仍然希望允许用户使用网站/服务。这意味着稍后要协调两组不同的数据，这既是一个技术挑战，也是一个业务风险。但通常技术挑战和业务风险都是可管理的，因此最好提供高可用性。

一致性和可用性并不是真正的二元选择，除非您将自己限制为强一致性。但是，强一致性只是一个一致性模型：在这个模型中，您必须放弃可用性，以防止多个数据副本处于活动状态。

简而言之：**“一致性”不是一个单一的，明确的属性**：

> [ACID](http://en.wikipedia.org/wiki/ACID) consistency != [CAP](http://en.wikipedia.org/wiki/CAP_theorem) consistency != [Oatmeal](http://en.wikipedia.org/wiki/Oatmeal) consistency

相反，一致性模型是数据存储向使用它的程序提供的保证——任何保证。

> Consistency model: a contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable

CAP中的“C”是“强一致性”，但“consistency”不是“strong consistency”的同义词。我们来看看一些替代的一致性模型。

## Strong consistency vs. other consistency models

一致性模型可以分为两类：强一致性模型和弱一致性模型：

* 强一致性模型（能够维护单个副本）
  * Linearizable consistency
  * Sequential consistency
* 弱一致性模型（不强）
  * Client-centric consistency models
  * Causal consistency: strongest model available
  * Eventual consistency models

强一致性模型保证更新的明显顺序和可见性等同于非复制系统。另一方面，弱一致性模型并不能做出这样的保证。

请注意，这绝不是一个详尽的清单。同样，一致性模型只是程序员和系统之间的一个契约，所以它们几乎可以是任何东西。

### Strong consistency models

强一致性模型可以进一步划分为两个相似但略有不同的一致性模型：

* Linearizable consistency：在Linearizable consistency下，所有操作似乎都按照与全局实时操作顺序一致的顺序原子执行。 
* Sequential consistency：在Sequential consistency下，所有操作似乎都以某种顺序原子执行，这与在各个节点上看到的顺序一致，并且在所有节点上都是相等的。

关键的区别在于，前者要求操作生效的顺序等于操作的实际实时顺序。后者允许操作重新排序，只要在每个节点上观察到的顺序保持一致。如果他们能够观察到进入系统的所有输入和时间，可以区分这两者——如果光从客户机与节点交互的角度来看，这两者是等效的。

差异似乎并不重要，但值得注意的是equential consistency并不构成。

强一致性模型允许您作为程序员用分布式节点集群替换单个服务器，而不会遇到任何问题。

所有其他一致性模型都有异常（与保证强一致性的系统相比），因为它们的行为方式可以与非复制系统区分开来。但这些异常现象通常是可以接受的，要么是因为我们不关心偶尔出现的问题，要么是因为我们编写了代码来处理在以某种方式发生不一致之后出现的问题。

请注意，弱一致性模型确实没有任何通用类型，因为“不是一个强一致性模型”（例如“在某种程度上可以与非复制系统区分开”）几乎可以是任何东西。

### Client-centric consistency models

以客户机为中心的一致性模型是以某种方式涉及客户机或会话概念的一致性模型。例如，以客户机为中心的一致性模型可以保证客户机永远不会看到数据项的旧版本。这通常是通过在客户端库中构建额外的缓存来实现的，这样，如果客户端移动到包含旧数据的副本节点，那么客户端库将返回其缓存值，而不是从副本返回旧值。

客户端仍然可以看到旧版本的数据，如果它们所在的副本节点不包含最新版本，但是它们永远不会看到旧版本的值重新出现的异常（例如，因为它们连接到不同的副本）。 请注意，有许多种以客户为中心的一致性模型。

### Eventual consistency

最终一致性模型表明，如果停止更改值，则在一些未定义的时间后，所有副本将同意相同的值。 暗示在此之前副本之间的结果在某种不确定的方式上是不一致的。 

说一些事情是最终一致的就像说“人们最终死了”。这是一个非常弱的约束，我们可能想要至少对两件事进行更具体的表征：

首先，“最终”有多长？具有严格的下限，或者至少对于系统收敛到相同值通常需要多长时间。

其次，副本如何就值达成一致？ 始终返回“42”的系统最终是一致的：所有副本都同意相同的值。 它只是没有收敛到有用的值，因为它只是保持返回相同的固定值。 相反，我们希望更好地了解该方法。 例如，一种决定方法是使具有最大时间戳的值始终获胜。

因此，当供应商说“最终一致”时，他们的意思是一些更精确的术语，例如“最终last-writer-wins，同时read-the-latest-observed-value”的一致性。 “how”是很重要的，因为糟糕的方法会导致写入丢失——例如，如果一个节点上的时钟设置不正确并且使用了时间戳。

我将在弱一致性模型的复制方法一章中更详细地研究这两个问题。

## Further reading

- [Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services](http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf) - Gilbert & Lynch, 2002
- [Impossibility of distributed consensus with one faulty process](http://scholar.google.com/scholar?q=Impossibility+of+distributed+consensus+with+one+faulty+process) - Fischer, Lynch and Patterson, 1985
- [Perspectives on the CAP Theorem](http://scholar.google.com/scholar?q=Perspectives+on+the+CAP+Theorem) - Gilbert & Lynch, 2012
- [CAP Twelve Years Later: How the "Rules" Have Changed](http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed) - Brewer, 2012
- [Uniform consensus is harder than consensus](http://scholar.google.com/scholar?q=Uniform+consensus+is+harder+than+consensus) - Charron-Bost & Schiper, 2000
- [Replicated Data Consistency Explained Through Baseball](http://pages.cs.wisc.edu/~remzi/Classes/739/Papers/Bart/ConsistencyAndBaseballReport.pdf) - Terry, 2011
- [Life Beyond Distributed Transactions: an Apostate's Opinion](http://scholar.google.com/scholar?q=Life+Beyond+Distributed+Transactions%3A+an+Apostate%27s+Opinion) - Helland, 2007
- [If you have too much data, then 'good enough' is good enough](http://dl.acm.org/citation.cfm?id=1953140) - Helland, 2011
- [Building on Quicksand](http://scholar.google.com/scholar?q=Building+on+Quicksand) - Helland & Campbell, 2009

# 3. Time and order

What is order and why is it important?

What do you mean "what is order"?

我的意思是，为什么我们一开始就如此关注于顺序？为什么我们关心A是否发生在B之前？为什么我们不关心其他的属性，比如“颜色”？

正如你可能记得的，我将分布式编程描述为：

> The art of solving the same problem that you can solve on a single computer using multiple computers.

事实上，这就是为什么分布式编程如此关注顺序。任何一次只能做一件事的系统都将创建一个操作的总顺序。就像人们通过一扇门一样，每一个行动都将有一个明确的前驱和后继者。这基本上就是我们努力维护的编程模型。

传统的模式是：在一个CPU上运行一个程序、一个进程、一个内存空间。操作系统抽象出这样一个事实：可能有多个CPU和多个程序，并且计算机上的内存实际上在许多程序之间共享。我不是说线程编程和面向事件的编程不存在；只是它们是“one/one/one”模型之上的特殊抽象。程序是按顺序执行的：从顶部开始，然后向下进入底部。

顺序作为一种属性受到了如此多的关注，因为定义“正确性”的最简单方法是说“它像在一台机器上那样工作”。这通常意味着a）我们运行相同的操作，b）我们以相同的顺序运行它们——即使有多台机器。

保持顺序的分布式系统（定义为单个系统）的好处在于它们是通用的。不需要关心这些操作是什么，因为它们将像在一台机器上一样执行。这很好，因为这样无论操作是什么，都可以使用相同的系统。

实际上，一个分布式程序在多个节点上运行；有多个CPU和多个操作流进入。你仍然可以分配一个总订单，但它需要精确的时钟或某种形式的通信。可以使用一个完全精确的时钟为每个操作设置时间戳，然后使用它计算出总顺序。或者，可能有某种通信系统，它可以按总顺序分配序列号。

## Total and partial order

分布式系统的自然状态是偏序的。无论是网络节点还是独立节点，都不能保证相对顺序；但是在每个节点上，都可以观察到本地顺序。

全序是一个二进制关系，它定义了某个集合中每个元素的顺序。当其中一个元素大于另一个元素时，两个截然不同的元素是可比较的。在偏序的集合中，一些元素对是不可比较的，因此偏序并不指定每个项的确切顺序。

全序和偏序都具有传递性和反对称性。对于X中的所有A、B和C，以下语句同时具有全序和偏序：

```
If a ≤ b and b ≤ a then a = b (反对称性);
If a ≤ b and b ≤ c then a ≤ c (传递性);
```

但是，全序符合整体性：

```
a ≤ b or b ≤ a (整体性) for all a, b in X
```

偏序只符合自反性：

```
a ≤ a (自反性) for all a in X
```

请注意，整体性意味着自反性；因此部分顺序是整体顺序的较弱变体。对于偏序的某些元素，totality属性不成立——换句话说，有些元素是不可比的。

Git Branch就是偏序的一个例子：

```
[ branch A (1,2,0)]  [ master (3,0,0) ]  [ branch B (1,0,2) ]
[ branch A (1,1,0)]  [ master (2,0,0) ]  [ branch B (1,0,1) ]
                  \  [ master (1,0,0) ]  /
```

分支A和分支B源于一个共同的祖先，但它们之间没有明确的顺序：它们代表不同的历史，如果没有额外的工作（合并），就不能简化为单一的线性历史。

在一个由一个节点组成的系统中，全序是必要的：指令被执行，消息在一个程序中以一个特定的、可观察的顺序被处理。于是我们开始依赖于这个全序——它使程序的执行具有可预测性。这种顺序可以在分布式系统上维护，但代价是：通信成本高昂，时间同步困难且脆弱。

## What is time?

时间是顺序之源——它允许我们定义操作的顺序——巧合的是，它也有一种人们可以理解的解释（一秒钟、一分钟、一天等等）。

在某种意义上，时间就像其他整数计数器一样。它恰好足够重要，大多数计算机都有一个专用的时间传感器，也就是时钟。这是非常重要的，我们已经找到了如何用一些不完善的物理系统（从蜡烛到铯原子）合成相同计数器的近似值。通过“综合”，我的意思是我们可以通过一些物理性质，在物理上遥远的地方近似整数计数器的值，而不需要直接通信。

时间戳实际上是表示从宇宙开始到当前时刻的世界状态的一个速记值——如果某个事件发生在某个特定的时间戳上，那么它可能受到之前发生的所有事情的影响。这个想法可以概括为一个因果时钟，它明确地跟踪原因（依赖性），而不是简单地假设时间戳之前的所有内容都是相关的。当然，通常的假设是，我们只应该担心特定系统的状态，而不是整个世界。

假设时间在任何地方都以相同的速度进行——这是一个很大的假设，我稍后将回到这个假设——时间和时间戳在程序中使用时有几个有用的解释。这三种解释是：

* 顺序
* 持续时间
* 解释

顺序。当我说时间是顺序之源，我的意思是：

* 我们可以将时间戳附加到无序事件从而使它们有序；
* 我们可以使用时间戳来强制执行特定的操作顺序或消息的传递（例如，如果操作无序到达则延迟操作）；
* 我们可以使用时间戳的值来确定某事物是否在其他事物之前按时间顺序发生。

解释。时间是一个普遍可比的价值。时间戳的绝对值可以解释为日期，这对人们很有用。

持续时间。持续时间与现实世界有一定的关系。算法通常不关心时钟的绝对值或它作为日期的解释，但它们可能会使用持续时间来作出一些判断。特别是，等待所花费的时间量可以提供有关线索来判断系统是分区的还是仅仅经历高延迟。

就其性质而言，分布式系统的组件的行为不可预测。它们不保证任何特定的顺序、增长率或延迟。每个节点都有一些本地命令——因为执行是（大致）连续的——但是这些本地命令彼此独立。

当事情可以以任何顺序发生时，人类很难对事情进行推理——只是有太多的排列需要考虑。

## Does time progress at the same rate everywhere?

我们都有一个直观的时间概念，基于我们个人的经验。不幸的是，直观的时间概念使我们更容易描绘出总序而不是偏序。更容易想象事情发生的顺序，一个接一个，而不是同时发生。对一个消息顺序进行推理要比对以不同顺序和不同延迟到达的消息进行推理容易得多。

然而，在实施分布式系统时，我们希望避免对时间和顺序做出强有力的假设，因为假设越强，系统就越容易受到“时间传感器”或车载时钟的问题的影响。此外，执行命令也会带来成本。我们越能容忍时间上的不确定性，就越能利用分布式计算。

“每个地方的时间增长的速度都相同吗？”这个问题有3个答案。这些是：

- "Global clock": yes
- "Local clock": no, but
- "No clock": no!

这些大致符合我在第二章中提到的三个计时假设：同步系统模型有一个全局时钟，部分同步模型有一个本地时钟，在异步系统模型中，根本不能使用时钟。让我们更详细地看看这些。

### Time with a "global-clock" assumption

全球时钟的假设是有一个完全准确的全球时钟，并且每个人都可以使用该时钟。这是我们考虑时间的方式，因为在人类互动中，时间上的微小差异并不真正重要。

![](http://book.mixu.net/distsys/images/global-clock.png)

全局时钟基本上是全序的（所有节点上每个操作的确切顺序，即使这些节点从未通信过）。

然而，这是一个理想化的世界观：在现实中，时钟同步只能在有限的精度范围内实现。这一点受到商品计算机时钟精度不足、使用NTP等时钟同步协议时的延迟以及时空本质的限制。

假设分布式节点上的时钟是完全同步的，这意味着假设时钟以相同的值开始，并且永不分离。这是一个很好的假设，因为可以自由地使用时间戳来确定一个由时钟漂移而不是延迟约束的全局总顺序，但这是一个非常重要的操作挑战，也是一个潜在的异常源。有许多不同的场景，其中一个简单的故障——例如用户意外地更改了机器上的本地时间，或者过时的机器加入了一个集群，或者同步时钟以稍微不同的速率漂移，等等，这可能导致难以跟踪的异常。

然而，有一些现实世界的系统做出了这个假设。Facebook的[Cassandra](http://en.wikipedia.org/wiki/Apache_Cassandra)就是一个假设时钟是同步的系统的例子。它使用时间戳来解决写入之间的冲突——使用新时间戳的写入操作将获胜。这意味着如果时钟漂移，新数据可能会被旧数据忽略或覆盖；同样，这是一个操作上的挑战（从我所听到的，人们敏锐地意识到的）。另一个有趣的例子是谷歌的Spanner：[本文](http://research.google.com/archive/spanner.html)描述了他们的TrueTimeAPI，它可以同步时间，但也可以估计最坏情况下的时钟漂移。

### Time with a "Local-clock" assumption

第二种可能更合理的假设是，每台机器都有自己的时钟，但没有全球时钟。这意味着不能使用本地时钟来确定远程时间戳是在本地时间戳之前还是之后发生的；换句话说，不能有意义地比较来自两台不同机器的时间戳。

![](http://book.mixu.net/distsys/images/local-clock.png)

本地时钟假设更接近于现实世界。它分配了一个偏序：每个系统上的事件都是有序的，但是不能只用一个时钟来跨系统对事件进行排序。

但是，可以使用时间戳在单台计算机上订购事件。当然，在终端用户控制的机器上，假设太多了：例如，用户可能在使用操作系统的日期控件查找日期时意外地将其日期更改为其他值。

### Time with a "No-clock" assumption

最后，还有逻辑时间的概念。在这里，我们根本不使用时钟，而是以其他方式跟踪因果关系。记住，时间戳只是到世界状态某一瞬间的简写，因此我们可以使用计数器和通信来确定是否发生了什么事情，是之前、之后还是同时发生了什么事情。

通过这种方式，我们可以确定不同机器之间事件的顺序，但不能谈论间隔，也不能使用超时（因为我们假设没有“时间传感器”）。这是偏序的：事件可以使用计数器在单个系统上进行排序，而无需通信，但跨系统排序事件需要消息交换。

在分布式系统中引用最多的论文之一是Lamport关于时间、时钟和事件顺序的论文。矢量时钟，这一概念的概括（我将更详细地介绍），是一种不用时钟跟踪因果关系的方法。Cassandra的堂兄弟Riak（Basho）和Voldemort（LinkedIn）使用矢量时钟，而不是假设节点可以访问一个完全精确的全局时钟。这使得这些系统可以避免前面提到的时钟精度问题。

当不使用时钟时，可以在远程机器上对事件进行排序的最大精度受通信延迟的限制。

## How is time used in a distributed system?

时间有什么好处？

* 时间可以定义整个系统的顺序（无通信）
* 时间可以定义算法的边界条件

事件顺序在分布式系统中很重要，因为分布式系统的许多属性是根据操作/事件的顺序定义的：

* 正确性取决于（协议）正确的事件排序，例如分布式数据库中的可序列化
* 当资源争用发生时，可以将时间顺序用作判断，例如，如果窗口小部件有两个订单，则执行第一个订单并取消第二个订单

全球时钟将允许在两台不同的机器上进行操作，而不需要两台机器直接通信。如果没有全球时钟，我们需要通信以确定顺序。

时间还可以用来定义算法的边界条件——特别是区分“高延迟”和“服务器或网络链路关闭”。这是一个非常重要的用例；在大多数实际系统中，超时用于确定远程计算机是否发生故障，或者它是否只是经历了高网络延迟。做出这一决定的算法称为故障检测器；我将很快讨论它们。

## Vector clocks (time for causal order)

前面，我们讨论了关于分布式系统中时间进度的不同假设。假设我们不能实现精确的时钟同步——或者从我们的系统不应该对时间同步问题敏感的目标开始，我们如何排序？

LAMPORT时钟和矢量时钟是物理时钟的替代品，它们依靠计数器和通信来确定分布式系统中事件的顺序。这些时钟提供了一个在不同节点之间可比较的计数器。

Lamport时钟很简单。每个进程使用以下规则维护计数器：

* 只要一个进程工作，就增加计数器
* 每当进程发送消息时，计数器也会跟着发送
* 收到消息时，将计数器设置为 `max(local_counter, received_counter) + 1`

表达为代码：

```
function LamportClock() {
  this.value = 1;
}

LamportClock.prototype.get = function() {
  return this.value;
}

LamportClock.prototype.increment = function() {
  this.value++;
}

LamportClock.prototype.merge = function(other) {
  this.value = Math.max(this.value, other.value) + 1;
}
```

一个lamport时钟允许在系统间比较计数器，但要注意：lamport时钟定义了一个偏序。如果 `timestamp(a) < timestamp(b)`：

* a可能发生在b之前或
* a可能与b无法比较

这被称为时钟一致性条件：如果一个事件先于另一个事件，那么该事件的逻辑时钟先于其他事件。如果a和b来自同一因果历史，例如，两个时间戳值都是在同一进程上生成的；或者b是对a中发送的消息的响应，那么我们知道a发生在b之前。

直观地说，这是因为lamport时钟只能携带一个时间线/历史的信息；因此，比较从不相互通信的系统的lamport时间戳可能会导致并发事件在不进行通信时看起来是有序的。

想象一下这样一个系统，它在一个初始阶段之后分成两个独立的子系统，这些子系统从不相互通信。

对于每个独立系统中的所有事件，如果a发生在b之前，则 `ts(a) < ts(b)`；但如果从不同独立系统中选取两个事件（例如，与因果关系无关的事件），则不能对它们的相对顺序说任何有意义的话。虽然系统的每个部分都为事件分配了时间戳，但这些时间戳彼此之间没有关系。两个事件似乎是有序的，即使它们是无关的。	

然而，从一台机器的角度来看，这仍然是一个有用的属性，用 `ts(a)` 发送的任何消息都将收到一个用 `ts(b)` 发送的响应，该响应`> ts(a)`。

矢量时钟是lamport时钟的一个扩展，它保持一个数组 `[ t1, t2, ... ]` 共n个逻辑时钟——每个节点一个。每个节点在每个内部事件上将向量中自己的逻辑时钟增加一个，而不是增加一个公共计数器。因此，更新规则是：

* 只要进程工作，就增加向量中节点的逻辑时钟值
* 每当进程发送消息时，逻辑时钟的完整向量也会跟着发送
* 收到消息时：
  * 将向量中的每个元素更新为 `max(local, received)`
  * 增加表示矢量中当前节点的逻辑时钟值

表达为代码：

```
function VectorClock(value) {
  // expressed as a hash keyed by node id: e.g. { node1: 1, node2: 3 }
  this.value = value || {};
}

VectorClock.prototype.get = function() {
  return this.value;
};

VectorClock.prototype.increment = function(nodeId) {
  if(typeof this.value[nodeId] == 'undefined') {
    this.value[nodeId] = 1;
  } else {
    this.value[nodeId]++;
  }
};

VectorClock.prototype.merge = function(other) {
  var result = {}, last,
      a = this.value,
      b = other.value;
  // This filters out duplicate keys in the hash
  (Object.keys(a)
    .concat(b))
    .sort()
    .filter(function(key) {
      var isDuplicate = (key == last);
      last = key;
      return !isDuplicate;
    }).forEach(function(key) {
      result[key] = Math.max(a[key] || 0, b[key] || 0);
    });
  this.value = result;
};
```

下图展示了一个矢量时钟的逻辑：

![](http://book.mixu.net/distsys/images/vector_clock.svg.png)

三个节点（A、B、C）中的每一个都跟踪矢量时钟。当事件发生时，它们用矢量时钟的当前值进行时间戳。通过检查向量时钟，如 `{ A: 2, B: 4, C: 1 }` ，我们可以准确地识别（可能）影响该事件的消息。

矢量时钟的问题主要是它们需要每个节点一个条目，这意味着对于大型系统来说，它们可能会变得非常大。已经应用了各种技术来减小矢量时钟的大小（通过执行定期垃圾收集，或者通过限制大小来降低精度）。

我们已经研究了如何在没有物理时钟的情况下跟踪顺序和因果关系。现在，让我们看看如何使用持续时间进行截止。

## Failure detectors (time for cutoff)

如我前面所述，等待所花费的时间量可以提供一个系统是分区的还是仅仅经历高延迟的线索。在这种情况下，我们不需要假设一个完全精确的全球时钟——只要有一个足够可靠的本地时钟就足够了。

给定一个程序在一个节点上运行，它怎么能告诉远程节点失败了？在缺乏准确信息的情况下，我们可以推断在经过一段合理的时间后，一个没有响应的远程节点已经失败。

但什么是“合理的数额”？这取决于本地和远程节点之间的延迟。与其显式地指定具有特定值的算法（在某些情况下不可避免地是错误的），不如处理适当的抽象。

故障检测器是一种提取准确时间假设的方法。故障检测器使用心跳消息和计时器实现。程交换心跳消息。如果在超时发生之前没有收到消息响应，那么进程会怀疑另一个进程。

基于超时的故障检测器将承担过度激进（声明节点发生故障）或过度保守（检测崩溃需要很长时间）的风险。故障检测器需要多精确才能使其可用？

钱德拉等人（1996）在解决共识的背景下讨论故障检测器——这是一个特别相关的问题，因为它是大多数复制问题的基础，其中复制副本需要在具有延迟和网络分区的环境中一致。

它们使用两个特性来描述故障检测器，即完整性和准确性：

> Strong completeness: Every crashed process is eventually suspected by every correct process.
>
> Weak completeness: Every crashed process is eventually suspected by some correct process.
>
> Strong accuracy: No correct process is suspected ever.
>
> Weak accuracy: Some correct process is never suspected.

完整性比准确性更容易实现；事实上，所有重要的故障检测器都能做到这一点——所需要做的就是不要永远怀疑某个进程。注意，具有弱完整性的故障检测器可以转换为具有强完整性的故障检测器（通过广播有关可疑过程的信息），从而使我们能够集中关注准确性的频谱。

除非能够假设消息延迟有一个固定最大值，否则很难避免错误地怀疑正确的进程。这种假设可以在同步系统模型中进行，因此故障检测器在这种系统中可以非常精确。在不对消息延迟施加硬限制的系统模型下，故障检测最好的情况下最终会是准确的。

钱德拉等人表明即使是非常弱的故障检测器——最终弱故障检测器W（准确性弱+完整性弱）也可以用来解决共识问题。下图说明了系统模型与问题解决能力之间的关系：

![](http://book.mixu.net/distsys/images/chandra_failure_detectors.png)

如您所见，在异步系统中，没有故障检测器，某些问题是无法解决的。这是因为如果没有故障检测器（或对时间界限的强假设，例如同步系统模型），就无法判断远程节点是否崩溃，或者只是经历了高延迟。这种崩溃或高延迟的区别对于任何旨在实现单一副本一致性的系统都很重要：失败的节点可以被忽略，因为它们不能导致分歧，但是分区节点不能被安全地忽略。

如何实现故障检测器？从概念上讲，对于一个简单的故障检测器来说并没有什么，它只在超时结束时检测故障。最有趣的部分是如何判断远程节点是否失败。

理想情况下，我们希望故障检测器能够适应不断变化的网络条件，并避免将超时值硬编码到其中。例如，Cassandra使用应计故障检测器，它是一个故障检测器，输出怀疑级别（介于0和1之间的值），而不是二进制“向上”或“向下”判断。这允许使用故障检测器的应用程序自行决定准确检测和早期检测之间的权衡。

## Time, order and performance

早些时候，我暗示必须支付顺序费用。什么意思？

如果你正在编写分布式系统，那么你可能拥有多台计算机。世界的自然（和现实）观是一个偏序，而不是一个全序。可以将一个部分顺序转换为一个全序，但这需要通信、等待并施加限制，以限制有多少计算机可以在任何特定时间点工作。

所有时钟都只是网络延迟（逻辑时间）或物理限制的近似值。即使在多个节点之间保持一个简单的整数计数器同步也是一个挑战。

虽然时间和顺序经常在一起讨论，但时间本身并不是一个有用的属性。算法并不真正关心时间，而是关心更抽象的属性：

* 事件的因果排序
* 故障检测（例如，消息传递的上限近似值）
* 一致的快照（例如，在某个时间点检查系统状态的能力;此处未讨论）

实施一个全面的命令是可能的，但代价昂贵。它要求你以共同的（最低的）速度前进。通常，确保以某种定义的顺序传递事件的最简单方法是指定一个（瓶颈）节点，通过该节点传递所有操作。

时间/顺序/同步性真的有必要吗？这要看情况而定。在某些用例中，我们希望每个中间操作将系统从一个一致状态移动到另一个一致状态。例如，在许多情况下，我们希望来自数据库的响应表示所有可用信息，并且希望避免处理如果系统返回不一致的结果可能发生的问题。

但在其他情况下，我们可能不需要那么多时间/顺序/同步。例如，如果你运行的是长时间运行的计算，并且直到最后才真正关心系统的工作，那么只要能够保证答案是正确的，就不需要太多的同步。

同步通常作为一种钝性工具应用于所有的操作，当只有一个子集的情况实际上对最终结果很重要时。何时需要顺序来保证正确性？我将在最后一章讨论的CALM定理提供一个答案。

在其他情况下，给出一个估计的最好答案是可以接受的——也就是说，只基于系统中包含的全部信息的一个子集。特别是，在网络分区期间，可能需要在系统的一部分可访问的情况下回答查询。在其他用例中，最终用户甚至无法真正区分可以便宜获得的相对较新的答案和保证正确且计算成本较高的答案之间的区别。例如，某个用户X或X 1的Twitter关注者数量是多少？ 或者电影A，B和C是一些查询的绝对最佳答案？ 做一个更便宜，最正确的“尽力而为”是可以接受的。

在接下来的两章中，我们将研究容错、强一致性系统的复制，这些系统在不断增强对故障的恢复能力的同时提供了强大的保证。当你需要保证正确性并愿意为此付出代价时，这些系统为第一种情况提供了解决方案。然后，我们将讨论具有弱一致性保证的系统，这些保证在分区面前仍然可用，但这只能给你一个“尽力而为”的答案。

## Further reading

### Lamport clocks, vector clocks

- [Time, Clocks and Ordering of Events in a Distributed System](http://research.microsoft.com/users/lamport/pubs/time-clocks.pdf) - Leslie Lamport, 1978

### Failure detection

- [Unreliable failure detectors and reliable distributed systems](http://scholar.google.com/scholar?q=Unreliable+Failure+Detectors+for+Reliable+Distributed+Systems) - Chandra and Toueg
- [Latency- and Bandwidth-Minimizing Optimal Failure Detectors](http://www.cs.cornell.edu/people/egs/sqrt-s/doc/TR2006-2025.pdf) - So & Sirer, 2007
- [The failure detector abstraction](http://scholar.google.com/scholar?q=The+failure+detector+abstraction), Freiling, Guerraoui & Kuznetsov, 2011

### Snapshots

- [Consistent global states of distributed systems: Fundamental concepts and mechanisms](http://scholar.google.com/scholar?q=Consistent+global+states+of+distributed+systems%3A+Fundamental+concepts+and+mechanisms), Ozalp Babaogly and Keith Marzullo, 1993
- [Distributed snapshots: Determining global states of distributed systems](http://scholar.google.com/scholar?q=Distributed+snapshots%3A+Determining+global+states+of+distributed+systems), K. Mani Chandy and Leslie Lamport, 1985

### Causality

- [Detecting Causal Relationships in Distributed Computations: In Search of the Holy Grail](http://www.vs.inf.ethz.ch/publ/papers/holygrail.pdf) - Schwarz & Mattern, 1994
- [Understanding the Limitations of Causally and Totally Ordered Communication](http://scholar.google.com/scholar?q=Understanding+the+limitations+of+causally+and+totally+ordered+communication) - Cheriton & Skeen, 1993

# 4. Replication

复制问题是分布式系统中的许多问题之一。我选择把重点放在其他问题上，如leader选择、失败检测、互斥、共识和全局快照，因为这往往是人们最感兴趣的部分。例如，区分并行数据库的一种方法是根据其复制特性。此外，复制为许多子问题提供了上下文，例如leader选择、故障检测、共识和原子广播。

复制是一个组通信问题。什么样的协议和通信模式能给我们期望的性能和可用性特征？面对网络分区和同时发生的节点故障，我们如何确保容错性、耐久性和无二义性？

让我们首先定义复制。我们假设我们有一些初始数据库，并且客户机发出更改数据库状态的请求。

![replication](http://book.mixu.net/distsys/images/replication-both.png)

然后可以将协议和通信模式分为几个阶段：

* （请求）客户端向服务器发送请求；
* （同步）复制的同步部分发生
* （响应）响应返回给客户端
* （异步）复制的异步部分发生

注意，在任务的每个部分中交换消息的模式取决于特定的算法，再次，我有意尝试在不讨论特定算法的情况下进行解释。

考虑这些阶段，我们能创造什么样的通信模式？我们选择的模式对性能和可用性的影响是什么？

## Synchronous replication

第一种模式是同步复制（也被称为active复制、eager复制、push复制或悲观复制）。让我们画出它的样子：

![replication](http://book.mixu.net/distsys/images/replication-sync.png)

在这里，我们可以看到三个不同的阶段：首先，客户机发送请求。接下来，我们称之为复制的同步部分发生了。这个术语指的其实是客户机被阻塞——它要等待来自系统的回复。

在同步阶段，第一个服务器与其他两个服务器联系，并等待，直到收到所有其他服务器的响应。最后，它向客户机发送一个响应，通知客户结果（例如成功或失败）。

这一切似乎都很简单。在不讨论同步阶段算法的细节的情况下，对于这种特定的通信模式协议，我们能注意到什么？首先，注意这是一种按N/N写的方法：在返回响应之前，系统中的每个服务器都必须看到并确认它。

从性能的角度来看，这意味着系统将和其中最慢的服务器一样快。系统还将对网络延迟的变化非常敏感，因为它要求每个服务器在继续操作之前都进行响应。

考虑到N/N的方法，系统不能容忍任何服务器的丢失。当服务器丢失时，系统将无法再向所有节点写入数据，因此无法继续。它可能能够提供对数据的只读访问，但在此设计中节点失败后不允许进行修改。

这种协议可以提供非常强的持久性保证：客户端可以确保在返回响应时，所有N个服务器都已接收、存储和确认请求。为了丢失一个已接受的更新，所有n个副本都需要丢失，这是一个尽可能好的保证。

## Asynchronous replication

让我们将其与第二种模式进行对比——异步复制（也就是被动复制、pull复制或lazy复制）。如你所料，这与同步复制相反：

![replication](http://book.mixu.net/distsys/images/replication-async.png)

在这里，主服务器（/leader/coordinator）会立即向客户机发送响应。它可能最多在本地存储更新，但不会同步地执行任何重要的工作，并且客户机不会被迫等待服务器之间发生更多轮的通信。

在稍后的某个阶段，复制任务的异步部分将发生。在这里，主服务器使用某种通信模式与其他服务器联系，其他服务器更新其数据副本。具体情况取决于使用的算法。

在不了解算法细节的情况下，我们能对这种特定的协议说些什么呢？好吧，这是一种1/N写的方法：响应立即返回，更新传播在稍后发生。

从性能的角度来看，这意味着系统是快速的：客户机不需要花费任何额外的时间来等待系统的内部完成他们的工作。系统对网络延迟的容忍度也更高，因为内部延迟的波动不会导致客户端的额外等待。

这种协议只能提供弱的耐久性保证。如果没有任何问题，数据最终会复制到所有n台机器上。但是，如果只有包含数据的服务器在发生这种情况之前丢失，则数据将永久丢失。

如果采用1/N的方法，只要至少有一个节点打开，系统就可以保持可用（至少在理论上是这样，尽管在实践中，负载可能太高）。这样一种纯粹的懒惰方法不提供持久性或一致性保证；可以允许你向系统写入数据，但不能保证在出现任何错误时可以读回所写的内容。

最后，值得注意的是，被动复制不能确保系统中的所有节点始终包含相同的状态。如果接受多个位置的写入，并且不要求这些节点同步约定，那么你将面临分歧的风险：读取可能会从不同位置返回不同的结果（尤其是在节点失败和恢复之后），并且无法强制执行全局约束（需要与所有节点通信）。

我没有真正提到读（而不是写）过程中的通信模式，因为读的模式实际上是遵循写的模式：在读过程中，你希望联系尽可能少的节点。我们将在引言的上下文中对此进行更多的讨论。

我们只讨论了两种基协议，没有涉及具体的算法。然而，我们已经能够对可能的通信模式以及它们的性能、耐久性保证和可用性特征进行相当多的了解。

## An overview of major replication approaches

在讨论了两种基本的复制方法：同步复制和异步复制之后，让我们看看主复制算法。

对复制技术进行分类有很多种不同的方法。我想介绍的第二个区别（在同步与异步之后）是：

* 防止分歧的复制方法（单副本系统）和
* 有分歧风险的复制方法（多主副本系统）

第一组方法具有“表现得像一个系统”的属性。特别是，当发生部分故障时，系统确保只有系统的一个副本处于活动状态。此外，系统确保副本始终一致。这就是所谓的共识问题。

如果几个计算机（或节点）都同意一些价值，那么它们就会达成共识。更正式地说：

- Agreement: Every correct process must agree on the same value.
- Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.
- Termination: All processes eventually reach a decision.
- Validity: If all correct processes propose the same value V, then all correct processes decide V.

互斥、leader选择、多播和原子广播都是更普遍的共识问题的实例。保持单一副本一致性的复制系统需要以某种方式解决一致性问题。

保持单一副本一致性的复制算法包括：

- 1n messages (asynchronous primary/backup)
- 2n messages (synchronous primary/backup)
- 4n messages (2-phase commit, Multi-Paxos)
- 6n messages (3-phase commit, Paxos with repeated leader election)

这些算法的容错性各不相同（例如，它们可以容忍的故障类型）。我只是根据算法执行过程中交换的消息数量对它们进行了分类，因为我认为尝试找到问题“我们通过增加消息交换的数量能获得什么？”的答案很有趣。

以下图表改编自Google的Ryan Barret，描述了不同选项的一些方面：

![Comparison of replication methods, from http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html](http://book.mixu.net/distsys/images/google-transact09.png)

上图中的一致性、延迟、吞吐量、数据丢失和故障转移特性实际上可以追溯到两种不同的复制方法：同步复制（例如，在响应之前等待）和异步复制。当你等待的时候，你会得到更差的性能，但更有力的保证。当我们讨论分区（和延迟）容忍度时，2PC和Quorum系统之间的吞吐量差异将变得明显。

在该图中，弱（即eventual）一致性的算法被集中到一个类别（“gossip”）。不过，我将更详细地讨论弱一致性（gossip和（部分）quorum系统）的复制方法。“transactions”实际上更多的是指全局谓词运算（如各种各样的运算符），这在一致性弱的系统中是不受支持的（尽管可以支持本地谓词运算）。

值得注意的是，执行弱一致性需求的系统具有更少的通用算法，以及更多可选择性应用的技术。由于不强制执行单一副本一致性的系统可以像由多个节点组成的分布式系统一样自由工作，因此要解决的明显目标较少，重点更多的是为人们提供一种方法来解释他们所拥有的系统的特性。

比如：

* 以客户端为中心的一致性模型试图在允许分歧的同时提供更易于理解的一致性保证。
* CRDT（收敛和交换复制数据类型）利用特定状态和基于操作的数据类型的半格属性（关联性、交换性、等幂性）。
* 合流分析（如Bloom语言）使用有关计算单调性的信息最大限度地利用无序性。
* PBS（概率有界过时）使用从现实系统收集的模拟和信息来描述部分quorum系统的预期行为。

我将进一步讨论所有这些问题。

## Primary/backup replication

主/备份复制（也称为主拷贝复制，主从复制或日志传送）可能是最常用的复制方法，也是最基本的算法。所有更新都在主服务器上执行，并且操作日志（或者更改）会通过网络发送到备份副本。有两种变体：

- asynchronous primary/backup replication 和
- synchronous primary/backup replication

同步版本需要两条消息（“更新”+“确认接收”），而异步版本只运行一条消息（“更新”）。

P/B（主从复制）很常见。例如，默认情况下，MySQL复制使用异步变量。MongoDB也使用P/B（带有一些用于故障转移的附加过程）。所有操作都在一个主服务器上执行，该主服务器将它们序列化为本地日志，然后将其异步复制到备份服务器。

正如我们前面在异步复制中讨论的，任何异步复制算法只能提供弱的持久性保证。在MySQL复制中，这表现为复制延迟：异步备份总是至少落后于主备份一个操作。如果主服务器发生故障，则尚未发送到备份的更新将丢失。

P/B复制的同步变体确保写操作在返回到客户机之前存储在其他节点上，而等待其他副本的响应则要付出代价。然而，值得注意的是，即使是这种变体也只能提供微弱的保证。考虑以下简单的故障场景：

* 主服务器接收写入并将其发送到备份；
* 备份持续存在并确认写入
* 然后在向客户端发送确认之前主服务器失败

客户机现在假定提交失败，但备份提交了；如果将备份升级到主服务器，则它将不正确。可能需要手动清理来协调失败的主备份或分散备份。

当然，我在这里简化了很多内容。虽然所有P/B复制算法都遵循相同的常规消息模式，但它们在故障转移处理、长时间脱机副本等方面有所不同。然而，在这个方案中，不可能对主系统的不适当故障有弹性。

在基于P/B的方案中，关键在于它们只能提供尽最大努力的保证（例如，如果节点在不适当的时间发生故障，它们很容易丢失更新或错误更新）。此外，P/B方案容易受到脑裂的影响，在这种情况下，由于临时网络问题而启动到备份的故障转移，并导致主备份和备份同时处于活动状态。

> **脑裂（split-brain）**:
>
> 指在一个高可用（HA）系统中，当联系着的两个节点断开联系时，本来为一个整体的系统，分裂为两个独立节点，这时两个节点开始争抢共享资源，结果会导致系统混乱，数据损坏。
>
> 对于无状态服务的HA，无所谓脑裂不脑裂；但对有状态服务(比如MySQL)的HA，必须要严格防止脑裂。（但有些生产环境下的系统按照无状态服务HA的那一套去配置有状态服务，结果可想而知...）

为了防止不适当的失败导致违反一致性保证，我们需要添加另一轮消息传递，这将使我们获得两阶段提交协议（2PC）。

## Two phase commit (2PC)

两阶段提交（2PC）是许多经典关系数据库中使用的协议。例如，mysql cluster（不要与普通mysql混淆）使用2pc提供同步复制，下图说明了消息流：

```
[ Coordinator ] -> OK to commit?     [ Peers ]
                <- Yes / No

[ Coordinator ] -> Commit / Rollback [ Peers ]
                <- ACK
```

在第一阶段（voting），Coordinator将更新发送给所有参与者。每个参与者处理更新并投票决定是提交还是中止。投票提交时，参与者将更新存储到临时区域（提前写入日志）。在第二阶段完成之前，更新被认为是临时的。

在第二阶段（决策），Coordinator决定结果，并通知每个参与者。如果所有参与者都投票同意提交，那么更新将从临时区域获取并永久更新。

在提交被认为是永久的之前,二阶段是有用的，因为它允许系统在节点失败时回滚更新。相反，在主/备份（“1PC”）中，没有步骤回滚在某些节点上失败而在其他节点上成功的操作，因此副本可能会出现分歧。

2PC很容易阻塞，因为单个节点故障（参与者或Coordinator）会阻塞进度，直到节点恢复。由于第二个阶段，恢复通常是可能的，在这个阶段中，其他节点会被告知系统状态。注意2PC假定每个节点的稳定存储中的数据永远不会丢失，并且不会永远崩溃。如果稳定存储中的数据在崩溃中损坏，数据仍然可能丢失。

节点故障期间恢复过程的详细信息非常复杂，因此我不想详细介绍。主要任务是确保对磁盘的写入是持久的（例如，刷新到磁盘而不是缓存），并确保做出正确的恢复决策（例如，学习每轮的结果，然后在本地重做或撤消更新）。

正如我们在关于CAP的章节中了解到的，2PC是一个CA——它不允许分区。2PC呈现的故障模型不包括网络分区；从节点故障恢复的指定方法是等待网络分区恢复。如果一个新的Coordinator失败了，就没有安全的方法来提升他；相反，需要人工干预。2PC还相当容易延迟，因为它是一种N/N的写方法，在最慢的节点确认之前，写操作无法继续。

2PC在性能和容错之间取得了相当好的平衡，这就是它在关系数据库中流行的原因。但是，较新的系统通常使用允许分区的一致性算法，因为这样的算法可以提供临时网络分区的自动恢复以及对增加的节点间延迟的更优雅的处理。

接下来让我们看一下分区容忍一致性算法。

## Partition tolerant consensus algorithms

分区容忍共识算法是我将要介绍的容错算法，它保持单拷贝一致性。还有一类容错算法：允许任意（拜占庭式）错误的算法；这些算法包括恶意操作导致失败的节点。这种算法很少在商业系统中使用，因为它们运行起来更昂贵，实现起来也更复杂——因此我将把它们排除在外。

当涉及到分区容忍共识算法时，最著名的算法是paxos算法。然而，众所周知，它很难实现和解释，所以我将重点关注raft，这是一种最近（~2013年初）的算法，旨在更易于教学和实现。让我们首先看一下网络分区和允许分区的共识算法的一般特性。

### What is a network partition?

网络分区是指到一个或多个节点的网络链接失败。节点本身继续保持活动状态，甚至可以从网络分区的客户端接收请求。正如我们之前所了解的，在讨论cap定理的过程中，网络分区发生后，并不是所有系统都能很好地处理它们。

网络分区很棘手，因为在网络分区期间，无法区分出现故障的节点和无法访问的节点。如果一个网络分区出现但没有节点失败，那么系统将被划分为两个同时处于活动状态的分区。下面的两个图说明了网络分区看起来如何类似于节点故障。

一个2节点的系统，具有故障与网络分区：

![replication](http://book.mixu.net/distsys/images/system-of-2.png)

一个3个节点的系统，具有故障与网络分区：

![replication](http://book.mixu.net/distsys/images/system-of-3.png)

一个强制实现单一拷贝一致性的系统必须有某种方法来打破对称性：否则，它将分裂成两个独立的系统，这两个系统可以彼此分离，并且不能再保持单一拷贝的假象。

对于强制执行单一副本一致性的系统，网络分区容错要求在网络分区期间，只有一个系统分区保持活动状态，因为在网络分区期间，不可能防止分裂（例如cap定理）。

### Majority decisions

这就是为什么分区容忍共识算法采用多数票的原因。只要求大多数节点（而不是所有节点，如2PC中的节点）同意更新，允许少数节点由于网络分区而停机、变慢或无法访问。只要 `(N/2 + 1)-of-N`  节点可以访问，系统就可以继续运行。

分区容忍共识算法使用奇数个节点（例如3、5或7）。只有两个节点，故障后就不可能有明显的多数。例如，如果节点数为3个，则系统对一个节点故障具有恢复能力；对于5个节点，系统对两个节点故障具有恢复能力。

当发生网络分区时，这些分区的行为是不对称的。一个分区将包含大多数节点。少数分区将停止处理操作，以防止网络分区期间出现分歧，但多数分区可以保持活动状态。这样可以确保只有一个系统状态副本保持活动状态。

大多数也很有用，因为它们可以容忍分歧：如果存在扰动或故障，节点的投票方式可能不同。然而，由于只可能有一个大多数决定，暂时的分歧最多只能阻止协议继续进行（放弃活跃性），但不会违反单一副本一致性标准（安全属性）。

### Roles

有两种方法可以构建一个系统：所有节点可能具有相同的职责，或者节点可能具有单独的、不同的角色。

复制的一致性算法通常选择对每个节点具有不同的角色。拥有一个固定的引导服务器或主服务器是一种优化，它使系统更高效，因为我们知道所有更新都必须通过该服务器。非主节点只需要将其请求转发给主节点。

注意，具有不同的角色并不妨碍系统从主节点（或任何其他角色）的失败中恢复。因为角色在正常操作期间是不变的，并不意味着在失败后重新分配角色（例如，通过leader选择阶段）就不能从失败中恢复。节点可以重用leader选择的结果，直到节点故障和/或网络分区发生为止。

paxos和raft都使用不同的节点角色。特别是，他们有一个领导节点（paxos中的提案者），负责在正常运行期间进行协调。在正常运行期间，其余节点是追随者（paxos中的“接受者”或“投票者”）。

### Epochs

在Paxos和Raft中，正常运行的每个阶段都被称为一个时代（epochs，Raft中称为term）。在每个时代，只有一个节点是指定的主节点（类似的系统在日本使用，在日本，时代名称随着帝国继承而改变）。

![replication](http://book.mixu.net/distsys/images/epoch.png)

选举成功后，领导者在任期间会协调，直到这个时代结束。如上图所示（来自筏纸），一些选举可能会失败，导致时代立即结束。

epoch充当一个逻辑时钟，允许其他节点识别过时节点何时开始通信——已分区或不工作的节点将具有比当前节点更小的epoch编号，并且它们的命令将被忽略。

### Leader changes via duels

在正常操作过程中，一个允许分区的共识算法相当简单。正如我们前面所看到的，如果我们不关心容错，我们可以只使用2PC。大部分的复杂性实际上来自于确保一旦做出一致决定，它就不会丢失，并且协议可以处理由于网络或节点故障而导致的主节点更改。

所有节点都以跟随者的身份开始；一个节点在开始时被选为领导者。在正常操作期间，领导者保持心跳，允许追随者检测领导者是否失败或被分割。

当一个节点检测到一个领导者变得不响应（或者，在最初的情况下，没有领导者存在），它会切换到一个中间状态（在raft中称为“候选人”），在这个状态中，它将时代值增加一个，启动一个领导者选举，并竞争成为新的领导者。

为了被选为领导者，一个节点必须获得大多数选票。分配选票的一种方法是简单地按先到先得的原则分配选票；这样，最终将选出一位领导人。在所有尝试当选之间的节点中添加随机的等待时间可以减少同时尝试当选的节点数。

### Numbered proposals within an epoch

在每一个时代，领袖提出一次一个value，以供表决。在每个时代，每个提案都用一个唯一的严格递增的数字编号。追随者（投票者/接受者）接受他们收到的针对特定提案编号的第一个提案。

### Normal operation

在正常运行期间，所有提案都将通过主节点。当客户机提交一个提案（例如更新操作）时，领导会联系quorum中的所有节点。如果不存在竞争性提案（基于追随者的回应），领导者会提出value。如果大多数追随者接受这个价值，那么这个价值就被认为是被接受的。

由于另一个节点也可能试图充当领导者，因此我们需要确保一旦接受了一个建议，它的value就永远不会改变。否则，一个已经被接受的提议可能会被竞争对手的领导回复。Lamport 这样表示：

> P2：如果选择了value为v的提案，则所选的每个编号较高的提案都具有v。

确保这一属性的持有，要求追随者和提议者都受到算法的约束，不能改变已被大多数人接受的value。请注意，“value永远不能更改”是指协议的单个执行（或运行/实例/决策）的value。一个典型的复制算法将运行该算法的多个执行，但大多数关于该算法的讨论集中在一次运行上，以保持简单。我们希望防止更改或覆盖决策历史记录。

为了强制执行此属性，提案人必须首先向追随者询问他们（编号最高）接受的提案和价值。如果提案人发现提案已经存在，那么它必须简单地完成本提案的执行，而不是自己提出提案。Lamport 这样表示：

> P2B。如果选择了具有值v的提议，则由任何提议者发布的每个更高编号的相同提议具有值v。

进一步来说：

> P2C。 对于任何v和n，如果[由领导者]发布具有值v和数字n的提议，则存在由大多数接受者[跟随者]组成的集合S，使得:
>
> （a）S中的接受者都没有接受 任何编号小于n的提案，或
>
> （b）v是S中编号所有小于N的所有提案里编号最高的那个提案的值。

这是paxos算法的核心，也是从中派生出来的算法。在协议的第二阶段之前，不会选择要提案的值。提案人有时必须简单地重新传输先前做出的决定，以确保安全（例如P2C中的条款b），直到他们知道自己可以自由地强加自己的提案价值（例如条款a）。

如果存在多个以前的提案，则使用编号最高的提案。只有在完全没有竞争性提案的情况下，提案人才能试图强加自己的值。

为了确保在提案人向每个接受人询问其最新值时不会出现竞争性提案，提案人要求跟随者不要接受提案编号低于当前提案编号的提案。

将各个部分放在一起，使用Paxos做出决定需要两轮沟通：

```
[ Proposer ] -> Prepare(n)                                [ Followers ]
             <- Promise(n; previous proposal number
                and previous value if accepted a
                proposal in the past)

[ Proposer ] -> AcceptRequest(n, own value or the value   [ Followers ]
                associated with the highest proposal number
                reported by the followers)
                <- Accepted(n, value)
```

准备阶段允许提案人了解任何竞争或以前的提案。第二阶段是提出新值或先前接受的值。在某些情况下，例如，如果两个提议者同时处于活动状态（决斗）；如果消息丢失；或者如果大多数节点都失败，则多数人不会接受任何提议。但这是可以接受的，因为提案的值的决策规则收敛到一个值（上一次尝试中建议数最高的值）。

事实上，根据FLP不可能的结果，这是我们所能做的最好的：解决共识问题的算法必须在消息传递边界的保证不成立时放弃安全性或活跃性。Paxos放弃了活跃性：它可能不得不无限期地推迟决策，直到某个时间点没有竞争的领导者，并且大多数节点都接受了一个提议。这比违反安全保证更可取。

当然，实现这个算法要比听起来困难得多。有许多小问题，即使是在专家的手中，加起来也相当可观的代码量。这些问题包括：

* 实际优化：
  * 通过领导租约（而不是心跳）避免重复领导人选举；
  * 避免在领导者份不变的稳定状态下重复建议消息；
* 确保追随者和提议者不会丢失稳定存储中的项目，并且存储在稳定存储中的结果不会被微妙地损坏（例如磁盘损坏）；
* 允许集群成员以安全的方式进行更改（例如，基本paxos依赖于这样一个事实：大多数成员总是在一个节点中相交，如果成员可以任意更改，那么这个节点就不起作用）；
* 需要有一个程序，在崩溃、磁盘丢失或配置新节点后，以安全和高效的方式更新新副本；
* 在一段合理的时间后，为确保安全所需的数据（例如，平衡存储要求和容错要求），需要有一个快照和垃圾数据收集程序。

Google的[Paxos Made Live](http://labs.google.com/papers/paxos_made_live.html)文章详细介绍了其中的一些挑战。

## Partition-tolerant consensus algorithms: Paxos, Raft, ZAB

希望你已经了解了一个允许分区的共识算法是如何工作的。我鼓励你阅读下阅读部分中的一篇文章，以了解不同算法的具体情况。

Paxos。 Paxos是编写强一致的分区容错复制系统时最重要的算法之一。 It is used in many of Google's systems, including the [Chubby lock manager](http://research.google.com/archive/chubby.html) used by [BigTable](http://research.google.com/archive/bigtable.html)/[Megastore](http://research.google.com/pubs/pub36971.html), the Google File System as well as [Spanner](http://research.google.com/archive/spanner.html).

Paxos通常被认为是很难实现的，并且有来自具有相当多分布式系统专业知识的公司的一系列论文解释了进一步的实际细节（请参阅进一步阅读）。You might want to read Lamport's commentary on this issue [here](http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#lamport-paxos) and [here](http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#paxos-simple).

这些问题主要涉及Paxos是根据一轮共识决策进行描述的事实，但实际工作实施通常希望有效地进行多轮共识。这导致了[核心协议上的许多扩展](http://en.wikipedia.org/wiki/Paxos_algorithm)的开发，任何对构建基于paxos的系统感兴趣的人仍然需要消化这些扩展。此外，还有一些额外的实际挑战，例如如何促进集群成员资格的改变。

ZAB。ZAB——在Apache ZooKeeper中使用了ZooKeeper原子广播协议。ZooKeeper是为分布式系统提供协调原语的系统，许多以Hadoop为中心的分布式系统（如HBase、Storm、Kafka）都使用它进行协调。ZooKeeper基本上是开源社区的Chubby版本。从技术上讲，原子广播是一个不同于纯共识的问题，但它仍然属于保证强一致性的分区容忍算法范畴。

Raft。RAFT是最近（2013年）对该算法系列的一个补充。它被设计成比Paxos更容易教学，同时提供相同的保证。特别是算法的不同部分之间的分离更加清晰，论文还描述了一种集群成员关系变化的机制。它最近在受ZooKeeper启发的[etcd](https://github.com/coreos/etcd)中被采用。

## Replication methods with strong consistency

在本章中，我们研究了强制强一致性的复制方法。从同步工作和异步工作之间的对比开始，我们逐步发展到能够容忍日益复杂的故障的算法。以下是每种算法的一些关键特性：

#### Primary/Backup

* Single, static master
* Replicated log, slaves are not involved in executing operations
* No bounds on replication delay
* Not partition tolerant
* Manual/ad-hoc failover, not fault tolerant, "hot backup"

#### 2PC

- Unanimous（一致） vote: commit or abort
- Static master
- 2PC cannot survive simultaneous failure of the coordinator and a node during a commit
- Not partition tolerant, tail latency sensitive

#### Paxos

- Majority vote
- Dynamic master
- Robust to n/2-1 simultaneous failures as part of protocol
- Less sensitive to tail latency

## Further reading

#### Primary-backup and 2PC

- [Replication techniques for availability](http://scholar.google.com/scholar?q=Replication+techniques+for+availability) - Robbert van Renesse & Rachid Guerraoui, 2010
- [Concurrency Control and Recovery in Database Systems](http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx)

#### Paxos

- [The Part-Time Parliament](http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf) - Leslie Lamport
- [Paxos Made Simple](http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf) - Leslie Lamport, 2001
- [Paxos Made Live - An Engineering Perspective](http://research.google.com/archive/paxos_made_live.html) - Chandra et al
- [Paxos Made Practical](http://scholar.google.com/scholar?q=Paxos+Made+Practical) - Mazieres, 2007
- [Revisiting the Paxos Algorithm](http://groups.csail.mit.edu/tds/paxos.html) - Lynch et al
- [How to build a highly available system with consensus](http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf) - Butler Lampson
- [Reconfiguring a State Machine](http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf) - Lamport et al - changing cluster membership
- [Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762) - Fred Schneider

#### Raft and ZAB

- [In Search of an Understandable Consensus Algorithm](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf), Diego Ongaro, John Ousterhout, 2013
- [Raft Lecture - User Study](http://www.youtube.com/watch?v=YbZ3zDzDnrw)
- [A simple totally ordered broadcast protocol](http://labs.yahoo.com/publication/a-simple-totally-ordered-broadcast-protocol/) - Junqueira, Reed, 2008
- [ZooKeeper Atomic Broadcast](http://labs.yahoo.com/publication/zab-high-performance-broadcast-for-primary-backup-systems/) - Reed, 2011

# 5. Replication: weak consistency model protocols

总的来说，很难想出一个单一维度来定义或表征允许副本分歧的协议。 大多数此类协议都是高度可用的，并且关键问题在于最终用户是否找到对其目的有用的保证，抽象和API，即使在发生节点和/或网络故障时副本可能会发散。

为什么没有弱一致的系统更受欢迎？

正如我在引言中所说，我认为大部分分布式编程都是关于处理分布的两个结果的影响：

* 信息以光速传播
* 独立的事情独立失败

信息传播速度的限制所产生的含义是，节点以不同的、独特的方式体验世界。在单个节点上计算很容易，因为所有事情都是以可预测的全局总顺序发生的。在分布式系统上计算是困难的，因为没有全局总阶。

在很长一段时间内（例如几十年的研究），我们通过引入全局全序来解决这个问题。我已经讨论了通过创建没有自然发生的总顺序的顺序（以容错方式）来实现强一致性的许多方法。

当然，问题是执行命令的成本很高。这种情况在大型互联网系统中尤为严重，因为系统需要保持可用性。强制强一致性的系统的行为不像分布式系统：它的行为像单个系统，这对分区期间的可用性不利。

此外，对于每个操作，通常必须联系大多数节点——而且通常不只是一次，而是两次（正如您在2PC讨论中看到的那样）。这在需要地理分布以为全球用户群提供足够性能的系统中尤其痛苦。

因此，默认情况下表现得像单个系统可能并不理想。

也许我们需要的是一个系统，在这个系统中，我们可以编写不使用昂贵协调的代码，但返回一个“可用”值。我们将允许不同的副本彼此分离，而不是只有一个事实，这既能保持效率，又能容忍分区，然后尝试以某种方式处理副本之间差异。

最终一致性表达了这样一个观点：节点可以在一段时间内彼此分离，但最终它们将在值上达成一致。在提供最终一致性的一组系统中，有两种类型的系统设计：

**概率保证的最终一致性**。这种类型的系统可以在以后的某个时间点检测到冲突的写入，但不能保证结果等同于某种正确的顺序执行。换句话说，冲突更新有时会导致用旧值覆盖新值，并且在正常操作（或分区）期间可能会出现一些异常。

近年来，提供单一副本一致性的最有影响力的系统设计是Amazon的Dynamo，我将以一个提供最终一致性和概率保证的系统为例进行讨论。

**强保证的最终一致性**。这种类型的系统保证结果收敛到一个公共值，相当于某种正确的顺序执行。换句话说，这样的系统不会产生任何异常结果；如果没有任何协调，您可以构建相同服务的副本，并且这些副本可以以任何模式通信并以任何顺序接收更新，并且只要它们都看到相同的信息，它们最终会在最终结果上达成一致。

CRDT（聚合复制数据类型）是一种数据类型，它可以保证在网络延迟、分区和消息重新排序的情况下收敛到相同的值。它们可以证明是收敛的，但是可以实现为CRDT的数据类型是有限的。

CALM（一致性如逻辑单调性）猜想是同一原理的另一种表达：它将逻辑单调性等同于收敛性。如果我们能得出这样的结论，即某事物在逻辑上是单调的，那么在没有协调的情况下运行也是安全的。合流分析——特别是应用于Bloom编程语言的合流分析——可用于指导程序员决定何时何地使用来自强一致系统的协调技术，以及在没有协调的情况下安全地执行协调技术。

## Reconciling different operation orders

不强制单一副本一致性的系统是什么样子的？让我们通过几个例子来尝试使这个更具体。

可能不强制单一副本一致性的系统最明显的特征是它们允许副本彼此分离。这意味着没有严格定义的通信模式：副本可以彼此分离，但仍然可以继续使用并接受写入。

让我们设想一个由三个副本组成的系统，每个副本都与其他副本分区。例如，复制副本可能位于不同的数据中心，并且由于某些原因无法通信。在分区期间，每个副本都保持可用，可以同时接受某些客户端集的读写操作：

```
[Clients]   - > [A]

--- Partition ---

[Clients]   - > [B]

--- Partition ---

[Clients]   - > [C]
```

一段时间后，分区恢复，副本服务器交换信息。他们从不同的客户那里收到了不同的更新，并且彼此之间存在分歧，因此需要进行某种协调。我们希望发生的是，所有的副本都收敛到相同的结果。

```
[A] \
    --> [merge]
[B] /     |
          |
[C] ----[merge]---> result
```

考虑具有弱一致性保证的系统的另一种方法是想象一组客户机以某种顺序向两个副本发送消息。由于没有强制执行单个总顺序的协调协议，因此消息可以在两个副本上以不同的顺序传递：

```
[Clients]  --> [A]  1, 2, 3
[Clients]  --> [B]  2, 3, 1
```

这本质上就是我们需要协调协议的原因。例如，假设我们试图连接一个字符串，消息1、2和3中的操作是：

```
1: { operation: concat('Hello ') }
2: { operation: concat('World') }
3: { operation: concat('!') }
```

然后，没有协调，A将产生"Hello World!"和B将产生"World!Hello "

```
A: concat(concat(concat('', 'Hello '), 'World'), '!') = 'Hello World!'
B: concat(concat(concat('', 'World'), '!'), 'Hello ') = 'World!Hello '
```

当然，这是不正确的。同样，我们希望发生的是，副本会收敛到相同的结果。

记住这两个例子，让我们先看一下亚马逊的Dynamo，以建立一个基线，然后讨论一些新的方法来构建具有弱一致性保证的系统，例如CRDT和CALME定理。

## Amazon's Dynamo

Amazon的Dynamo系统设计（2007）可能是最著名的系统，它提供了弱一致性保证，但高可用性。它是许多其他现实世界系统的基础，包括LinkedIn的Voldemort、Facebook的Cassandra和Basho的Riak。

Dynamo是一个最终一致、高度可用的键值存储库。键值存储就像一个大型哈希表：客户机可以通过 `set(key, value)` 设置值，并使用 `get(key)`按key检索值。一个dynamo集群由n个对等节点组成；每个节点都有一组键，这些键负责存储。

Dynamo将可用性优先于一致性；它不保证单拷贝一致性。相反，在写入值时，副本可能会彼此分离；读取密钥时，存在一个读取协调阶段，该阶段尝试在将值返回到客户端之前协调副本之间的差异。

对于Amazon上的许多功能，避免停机比确保数据完全一致更重要，因为停机会导致业务损失和信誉损失。此外，如果数据不是特别重要，那么弱一致性系统可以以比传统RDBMS更低的成本提供更好的性能和更高的可用性。

由于Dynamo是一个完整的系统设计，除了核心复制任务，还有许多不同的部分需要考虑。下面的图表说明了一些任务；特别是如何将写操作路由到节点并写入多个副本。

```
[ Client ]
    |
( Mapping keys to nodes )
    |
    V
[ Node A ]
    |     \
( Synchronous replication task: minimum durability )
    |        \
[ Node B]  [ Node C ]
    A
    |
( Conflict detection; asynchronous replication task:
  ensuring that partitioned / recovered nodes recover )
    |
    V
[ Node D]
```

在查看最初如何接受写入之后，我们将查看如何检测冲突以及异步副本同步任务。 由于高可用性设计，此任务是必需的，其中节点可能暂时不可用（向下或分区）。 副本同步任务可确保节点即使在发生故障后也能够快速赶上。

### Consistent hashing

无论我们是在读还是在写，首先需要做的是我们需要找到数据应该在系统上的位置。这需要某种类型的键到节点映射。

在Dynamo中，键使用一种称为一致散列的散列技术映射到节点（我不会详细讨论）。主要思想是，通过在客户机上进行简单的计算，可以将一个键映射到负责它的一组节点上。这意味着客户机可以定位键，而不必查询系统中每个键的位置；这节省了系统资源，因为哈希通常比执行远程过程调用更快。

### Partial quorums

一旦我们知道键应该存储在哪里，我们就需要做一些工作来持久化该值。这是一个同步任务；我们将立即将值写入多个节点的原因是为了提供更高的持久性（例如，防止节点立即发生故障）。

和Paxos或Raft一样，Dynamo使用Quorum（仲裁）进行复制。然而，Dynamo的quorums 是草率的（部分的）quorums ，而不是严格的（多数的）quorums 。

Informally, a strict quorum system is a quorum system with the property that any two quorums (sets) in the quorum system overlap. Requiring a majority to vote for an update before accepting it guarantees that only a single history is admitted since each majority quorum must overlap in at least one node. This was the property that Paxos, for example, relied on.

非正式地说，严格的quorum 制度是一种quorum 制度，它具有quorum 制度中任意两个quorum （集合）重叠的性质。要求大多数人在接受更新之前投票，这样可以保证只有一个历史被接受，因为每个多数仲裁必须至少在一个节点上重叠。例如，这就是Paxos所依赖的属性。

部分quorum 不具有该属性；这意味着不需要多数，并且quorum 的不同子集可能包含相同数据的不同版本。用户可以选择要写入和读取的节点数：

* 用户可以选择写入成功所需的一些W-of-N节点;
* 用户可以指定在读取期间要联系的节点数（R-of-N）

w和r指定一个写入或读取需要涉及的节点数。写入更多节点会使写入速度稍慢，但增加值不会丢失的概率；从更多节点读取会增加值读取最新的概率。

通常的建议是R+W>N，因为这意味着读写quorum在一个节点中重叠，从而减少返回过时值的可能性。典型配置为n=3（例如，每个值总共有三个副本）；这意味着用户可以在以下两种情况之间进行选择：

```
 R = 1, W = 3;
 R = 2, W = 2 or
 R = 3, W = 1
```

更一般地说，再次假设 `R + W > N`：

- `R = 1`, `W = N`: fast reads, slow writes
- `R = N`, `W = 1`: fast writes, slow reads
- `R = N/2` and `W = N/2 + 1`: favorable to both

N很少超过3，因为保留大量数据的大量副本变得昂贵！

正如我之前提到的，Dynamo论文启发了许多其他类似的设计。它们都使用相同的基于部分仲裁（quorum）的复制方法，但N，W和R的默认值不同：

- Basho's Riak (N = 3, R = 2, W = 2 default)
- Linkedin's Voldemort (N = 2 or 3, R = 1, W = 1 default)
- Apache's Cassandra (N = 3, R = 1, W = 1 default)

还有另一个细节：当发送读或写请求时，是否要求所有n个节点响应（RIAK），或者只要求满足最小值的一些节点（例如R或W；voldemort）。“发送到所有”方法更快，对延迟不太敏感（因为它只等待n个最快的r或w节点），但效率也较低，“发送到最小”方法对延迟更敏感（因为与单个节点通信的延迟会延迟操作），但效率也更高（总的来说，消息/连接更少））。

当读写仲裁重叠时会发生什么，例如（ `R + W > N`）？具体而言，经常声称这导致“强一致性”。

### Is R + W > N the same as "strong consistency"?

不。

它并不是完全脱离基础的：一个R+W>N可以检测读/写冲突的系统，因为任何读仲裁和任何写仲裁共享一个成员。例如，至少有一个节点位于两个仲裁中：

```
  1     2   N/2+1     N/2+2    N
  [...] [R]  [R + W]   [W]    [...]
```

这保证了后续读取将看到先前的写入。但是，只有当N中的节点永远不会改变时，这才成立。因此，Dynamo不符合条件，因为在Dynamo中，如果节点失败，集群成员可能会发生变化。

Dynamo设计为始终可写。它有一种机制，当原始服务器关闭时，通过将不同的、不相关的服务器添加到负责某些键的节点集中来处理节点故障。这意味着仲裁不再保证总是重叠。即使`r=w=n`也不符合条件，因为当仲裁大小等于n时，这些仲裁中的节点可能在失败时发生更改。具体来说，在分区期间，如果无法达到足够数量的节点，那么dynamo将从不相关但可访问的节点向仲裁添加新节点。

此外，Dynamo并没有像执行强一致性模型的系统那样处理分区：也就是说，在分区的两侧都允许写入，这意味着至少有一段时间系统不会作为单个副本。因此，将`r+w>n`称为“强一致性”是一种误导；这种保证只是概率性的——而不是强一致性所指的。

### Conflict detection and read repair

允许副本分散的系统必须有最终协调两个不同值的方法。正如在部分仲裁方法中简要提到的，实现这一点的一种方法是在读取时检测冲突，然后应用一些冲突解决方法。但这是怎么做到的？

一般来说，这是通过用一些元数据补充数据来跟踪一段数据的因果历史来完成的。客户端在从系统中读取数据时必须保留元数据信息，并且在写入数据库时必须返回元数据值。

我们已经遇到了这样做的方法：矢量时钟可以用来表示一个值的历史记录。实际上，这正是最初的Dynamo 设计用来检测冲突的地方。

然而，使用矢量时钟并不是唯一的选择。如果查看许多实际的系统设计，你可以通过查看它们跟踪的元数据来推断它们是如何工作的。

**没有元数据**。当系统不跟踪元数据，只返回值（例如通过客户端API）时，它就不能真正地对并发写入做任何特殊的事情。一个常见的规则是，最后一个写入者获胜：换句话说，如果两个写入者同时write，那么只有来自最慢写入者的值被保留。

**时间戳**。名义上，时间戳值越大的值就赢。然而，如果时间没有仔细同步，许多奇怪的事情会发生，在这些情况下，来自故障或快速时钟的系统的旧数据会覆盖新的值。Facebook的Cassandra是一个Dynamo 变种，它使用时间戳而不是矢量时钟。

**版本号**。版本号可能会避免一些与使用时间戳相关的问题。请注意，当可能存在多个历史时，能够准确跟踪因果关系的最小机制是矢量时钟，而不是版本号。

**向量时钟**。使用矢量时钟，可以检测到并发和过期的更新。然后就可以执行读修复了，尽管在某些情况下（并发更改），我们需要让客户机选择一个值。这是因为如果更改是并发的，并且我们对数据一无所知（就像简单的键值存储那样），那么请求比随意丢弃数据要好。

当读取一个值时，客户机联系n个节点中的r，并向它们请求键的最新值。它获取所有响应，丢弃严格较旧的值（使用矢量时钟值检测）。如果只有一个唯一的向量时钟+值对，则返回该值。如果同时编辑了多个矢量时钟+值对（例如不可比较），则返回所有这些值。

如上所述，read repair可能返回多个值。这意味着客户端/应用程序开发人员必须偶尔通过根据特定于用例的标准选择一个值来处理这些情况。

此外，一个实际的矢量时钟系统的一个关键组成部分是，时钟不能永远增长-因此需要有一个程序，偶尔以安全的方式垃圾收集时钟，以平衡容错性和存储要求。

### Replica synchronization: gossip and Merkle trees

考虑到Dynamo系统的设计能够容忍节点故障和网络分区，需要一种方法来处理分区后或故障节点被替换或部分恢复后重新加入集群的节点。

副本同步用于在发生故障后使节点保持最新状态，并用于定期相互同步副本。

Gossip 是同步副本的概率技术。通信模式（例如哪个节点与哪个节点接触）未事先确定。相反，节点有一些概率p试图彼此同步。每T秒，每个节点选择一个节点进行通信。这提供了同步任务（例如部分仲裁写入）之外的附加机制，使副本成为最新的。

Gossip 是可扩展的，没有单一的失败点，但只能提供概率保证。

为了使副本同步期间的信息交换效率更高，Dynamo使用了一种称为Merkle树的技术，我将不详细介绍它。关键思想是数据存储可以在多个不同的粒度级别进行哈希：表示整个内容的哈希、一半的键、四分之一的键等等。

通过保持这种相当精细的散列，节点可以比简单的技术更有效地比较数据存储内容。一旦节点识别出哪些键具有不同的值，它们就交换必要的信息以使副本保持最新。

### Dynamo in practice: probabilistically bounded staleness (PBS)

这几乎涵盖了Dynamo系统设计：

* 一致的散列来确定关键位置
* 读和写的部分仲裁
* 通过矢量时钟进行冲突检测和读取修复
* 用于复制同步的Gossip

我们如何描述这样一个系统的行为？一篇来自贝利等的近期论文。（2012）描述了一种称为PBS（概率有界过时）的方法，该方法使用从现实系统收集的模拟和数据来描述此类系统的预期行为。

PBS通过反熵（gossip）率、网络延迟和本地处理延迟等信息来估计读操作的一致性。它已经在Cassandra中实现，在Cassandra中，定时信息被附加在其他消息上，并根据蒙特卡洛模拟中的该信息样本计算估计值。

基于本文，在正常运行期间，一致性数据存储通常更快，并且可以在数十或数百毫秒内读取一致性状态。下表说明了根据LinkedIn（SSD和15K RPM磁盘）和Yammer的凭经验的定时数据的不同r和w设置，99.9%的一致读取概率所需的时间：

![from the PBS paper](http://book.mixu.net/distsys/images/pbs.png)

例如，在Yammer情况下，从r=1，w=1到r=2，w=1将不一致窗口从1352 ms减少到202 ms，同时保持读取延迟低于最快严格仲裁（r=3，w=1；219.27 ms）。

For more details, have a look at the [PBS website](http://pbs.cs.berkeley.edu/) and the associated paper.

## Disorderly programming

让我们回顾一下我们想要解决的各种情况的例子。第一个场景由分区后面的三个不同的服务器组成；在分区修复之后，我们希望服务器收敛到相同的值。Amazon的Dynamo通过从n个节点中的r读取数据，然后执行读取协调来实现这一点。

在第二个示例中，我们考虑了一个更具体的操作：字符串连接。事实证明，没有已知的技术可以使字符串连接解析为相同的值，而无需对操作施加命令（例如，没有昂贵的协调）。但是，有些操作可以按任何顺序安全地应用，而简单的寄存器就不能这样做。正如帕特·海兰德所写：

> 以操作为中心的工作可以交换（使用正确的操作和正确的语义），其中简单的READ / WRITE语义不适合交换。

例如，假设一个系统以两种不同的方式实现一个简单的会计系统，使用`debit` 操作和`credit` 操作：

* 使用具有读写操作的寄存器，以及
* 使用原生`debit` 操作和`credit` 操作的整数数据类型

后一种实现更了解数据类型的内部，因此它可以保留操作的意图，尽管操作被重新排序。`debit` 或`credit` 可按任何顺序应用，最终结果相同：

```
100 + credit(10) + credit(20) = 130 and
100 + credit(20) + credit(10) = 130
```

但是，无法按任何顺序写入固定值：如果重新排序写入，则其中一个写入操作将覆盖另一个：

```
100 + write(110) + write(130) = 130 but
100 + write(130) + write(110) = 110
```

让我们以本章开头的示例为例，但使用不同的操作。在这个场景中，客户机向两个节点发送消息，这些节点以不同的顺序查看操作：

```
[Clients]  --> [A]  1, 2, 3
[Clients]  --> [B]  2, 3, 1
```

假设我们正在寻找一组整数的最大值（例如max()），而不是字符串串联。消息1、2和3是：

```
1: { operation: max(previous, 3) }
2: { operation: max(previous, 5) }
3: { operation: max(previous, 7) }
```

然后，在没有协调的情况下，A和B都会收敛到7，例如：

```
A: max(max(max(0, 3), 5), 7) = 7
B: max(max(max(0, 5), 7), 3) = 7
```

在这两种情况下，两个副本以不同的顺序查看更新，但是我们能够以相同的方式合并结果，不管顺序是什么。由于我们使用的合并过程（max），结果在两种情况下都收敛到相同的答案。

很可能无法编写适用于所有数据类型的合并过程。在dynamo中，一个值是一个二进制blob，所以可以做的最好的事情就是公开它并让应用程序处理每个冲突。

但是，如果我们知道数据是更具体的类型，那么处理这些冲突就成为可能。CRDT是一种数据结构，旨在提供始终聚合的数据类型，只要它们看到相同的操作集（以任何顺序）。

## CRDTs: Convergent replicated data types

CRDT（收敛复制数据类型）利用有关特定数据类型上特定操作的交换性和关联性的知识。

为了使一组操作在副本只是偶尔通信的环境中聚合到相同的值上，操作需要与顺序无关，并且对（消息）复制/重新传递不敏感。因此，他们的操作需要：

* 关联 (`a+(b+c)=(a+b)+c`)，因此分组无关紧要
* 可交换(`a+b=b+a`)，所以应用顺序无关紧要
* 幂等 (`a+a=a`)，所以重复无关紧要

事实证明，这些结构在数学中已经是众所周知的；它们被称为join semilattice 或meet semilattice（半格）。

格子是部分有序的集合，具有不同的顶部（最小上限）和不同的底部（最大下限）。 半格就像一个格，但只有一个独特的顶部或底部。 join semilattice 是具有不同顶部（最小上边界）的半格，并且meet semilattice是具有不同底部（最大下边界）的半格。

任何可以表示为半格的数据类型都可以实现为保证收敛的数据结构。例如，计算一组值的`max()`将始终返回相同的结果，而不管接收值的顺序如何，只要最终接收到所有值，因为`max()`操作是关联的、交换的和等幂的。

例如，这里有两个格：一个为集合绘制图，其中合并运算符是`union(items)`，另一个为严格递增的整数计数器绘制图，其中合并运算符是 `max(values)`：

```
   { a, b, c }              7
  /      |    \            /  \
{a, b} {b,c} {a,c}        5    7
  |  \  /  | /           /   |  \
  {a} {b} {c}            3   5   7
```

对于可以表示为半格的数据类型，您可以让副本以任何模式进行通信，并以任何顺序接收更新，并且只要它们都看到相同的信息，它们最终将在最终结果上达成一致。这是一个强大的属性，只要前提条件成立，就可以得到保证。

然而，将数据类型表示为半格通常需要一定程度的解释。许多数据类型都具有实际上与顺序无关的操作。例如，向集合添加项是关联的、交换的和等幂的。但是，如果我们还允许从集合中删除项，那么我们需要某种方法来解决冲突操作，例如`add(a)`和`remove(a)`。如果本地副本从未添加元素，那么删除该元素意味着什么？必须以顺序独立的方式指定此解决方案，并且有几个不同的选择，具有不同的权衡。

这意味着，一些熟悉的数据类型具有更为专门的实现，如CRDT，它们可以进行不同的权衡，以独立于顺序的方式解决冲突。与只处理寄存器的键值存储不同（例如，从系统的角度看，这些值是不透明的blob），使用CRDT的人必须使用正确的数据类型以避免异常。

指定为CRDT的不同数据类型的一些示例包括：

* 计数器（Counters）：
  * 只增长计数器（合并=max(values)；payload=单个整数）
  * 正负计数器（由两个增长计数器组成，一个用于递增，另一个用于递减）
* 寄存器（Registers）：
  * Last Write Wins寄存器（timestamp或版本号；合并=max(ts)；payload=blob）
  * 多值寄存器（矢量时钟;合并=取两者）
* 集合（sets）：
  * Grow-only set (合并= union(items); payload = set; 无删除)
  * Two-phase set (consists of two sets, one for adding, and another for removing; elements can be added once and removed once)
  * Unique set (two-phase set的一个优化版本)
  * Last write wins set (merge = max(ts); payload = set)
  * 正负set（每个item包含一个正负计数器）
  * Observed-remove set
* 图和文本序列（见paper）

为了确保无异常操作，你需要为特定应用程序找到正确的数据类型-例如，如果您知道只删除一个项一次，则两阶段集有效；如果只向集添加项而从不删除，则仅增长集有效。

并非所有的数据结构都有已知的CRDT实现，但[夏皮罗等人最近（2011）的调查论文](http://hal.inria.fr/docs/00/55/55/88/PDF/techreport.pdf)中有针对布尔值、计数器、集合、寄存器和图的CRDT实现。

有趣的是，寄存器的实现与键值存储使用的实现直接对应：一个last-write-wins的wins寄存器使用时间戳或等效的时间戳，并简单地收敛到最大的时间戳值；多值寄存器对应于保留、公开和协调并发更改的dynamo策略。有关详细信息，我建议您查看本章后续阅读部分中的论文。

## The CALM theorem

CRDT的数据结构是基于对半格数据结构收敛的认识。但是编程不仅仅是进化状态，除非只是实现一个数据存储。

显然，顺序独立性是任何收敛计算的一个重要属性：如果数据项的接收顺序影响计算的结果，那么在不保证顺序的情况下就无法执行计算。

然而，有许多编程模型中语句的顺序不起重要作用。例如，在map reduce模型中，map和reduce任务都被指定为需要在数据集上运行的无状态元组处理任务。关于如何以及以何种顺序将数据路由到任务的具体决策没有明确指定，而是由批处理作业调度程序负责调度要在集群上运行的任务。

同样，在SQL中，一个指定查询，但不指定如何执行查询。查询只是对任务的一种声明性描述，查询优化器的工作就是找出一种有效的执行查询的方法（跨多台机器、数据库和表）。

当然，这些编程模型不像通用编程语言那样具有许可性。在非循环数据流程序中，mapreduce任务需要可以表示为无状态任务；SQL语句可以执行相当复杂的计算，但许多事情很难在其中表达。

然而，从这两个例子中应该清楚地看到，有许多类型的数据处理任务可以用一种声明性语言来表示，其中没有明确指定执行顺序。在将语句的确切顺序留给优化器决定的同时，表示所需结果的编程模型通常具有独立于顺序的语义。这意味着这些程序可能不需要协调就可以执行，因为它们取决于它们接收的输入，但不一定取决于接收输入的具体顺序。

关键是这些程序在没有协调的情况下可以安全地执行。如果没有一个明确的规则来描述没有协调什么是安全的，什么是不安全的，我们就不能在确保结果是正确的同时实现一个程序。

这就是CALM 定理的意义所在。CALM 定理是基于对逻辑单调性与最终一致性的有用形式（如合流/收敛）之间联系的认识。它指出，逻辑上单调的程序保证最终是一致的。

那么，如果我们知道某些计算在逻辑上是单调的，那么我们就知道在没有协调的情况下执行也是安全的。

为了更好地理解这一点，我们需要将单调逻辑（或单调计算）与非单调逻辑（或非单调计算）进行对比。

> Monotony(单调的): if sentence `φ` is a consequence of a set of premises `Γ`, then it can also be inferred from any set `Δ` of premises extending `Γ`

大多数标准逻辑框架都是单调的：框架内的任何推论，如一阶逻辑，一旦演绎有效，就不能被新的信息所推翻。非单调逻辑是一个系统，在这个系统中，如果一些结论可以通过学习新知识而无效，那么这个属性就不成立。

在人工智能领域，非单调逻辑与可推翻的推理——推理联系在一起，在这种推理中，利用部分信息作出的断言可能被新知识所推翻。例如，如果我们知道tweety是一只鸟，我们会假设tweety可以飞；但是如果我们后来知道tweety是一只企鹅，那么我们将不得不修改我们的结论。

单调性涉及前提（或关于世界的事实）和结论（或关于世界的断言）之间的关系。在单调逻辑中，我们知道我们的结果是无伸缩的：单调计算不需要重新计算或协调；随着时间的推移，答案会变得更准确。一旦我们知道tweety是一只鸟（并且我们用单调逻辑推理），我们就可以安全地得出结论，tweety可以飞起来，我们所学的任何东西都不能使这个结论无效。

虽然任何产生面向人类结果的计算都可以解释为对世界的断言（例如，“foo”的值是“bar”），但很难确定基于von neumann机器的编程模型中的计算是否单调，因为尚不清楚事实和断言之间的关系是什么，以及这些关系是否单调。

然而，有许多编程模型可以确定单调性。特别是，关系代数（例如，SQL的理论基础）和数据日志提供了高度表达性的语言，这些语言具有很好的解释能力。

基本数据日志和关系代数（即使是递归的）都是单调的。更具体地说，使用一组基本运算符表示的计算是单调的（选择、投影、自然联接、交叉积、并集和递归数据日志，无求反），使用更高级的运算符（求反、集差、除、通用量化、聚合）引入非单调性。）

这意味着，在这些系统中使用大量运算符（如map、filter、join、union、intersection）表示的计算在逻辑上是单调的；使用这些运算符的任何计算也都是单调的，因此在没有协调的情况下运行是安全的。另一方面，使用否定和聚合的表达式在没有协调的情况下运行是不安全的。

在分布式系统中，实现非单调性与代价高昂的操作之间的联系是非常重要的。具体来说，分布式聚合和协调协议都可以被认为是一种否定形式。正如JoeHellerstein所写：

> 为了在分布式设置中确定否定谓词的准确性，评估策略必须开始“计数到0”以确定空性，并等待分布式计数过程确定结束。聚合是这个想法的概括。

以及：

> 这个想法也可以从另一个方向看出来。协调协议本身就是聚合，因为它们需要投票：两阶段提交需要一致的投票，Paxos共识需要多数票，拜占庭协议需要2/3的多数票。等待需要计数。

如果，那么我们可以用一种可以测试单调性的方式来表达我们的计算，那么我们就可以执行一个完整的程序静态分析，检测程序的哪些部分最终是一致的，并且在没有协调的情况下运行是安全的（单调部分），哪些部分不是（非单调部分）。

请注意，这需要一种不同的语言，因为对于以序列、选择和迭代为核心的传统编程语言来说，很难做出这些推论。这就是Bloom语言被设计出来的原因。

## What is non-mononicity good for?

单调性和非单调性之间的区别很有趣。例如，添加两个数字是单调的，但是计算包含数字的两个节点上的聚合不是单调的。有什么区别？其中一个是计算（添加两个数字），另一个是断言（计算聚合）。

计算与断言有何不同？让我们考虑一下“比萨是蔬菜吗？”为了回答这个问题，我们需要找到核心：什么时候可以接受推断某件事是（或不是）真的？

有几个可接受的答案，每一个都对应于一组不同的假设，关于我们拥有的信息以及我们应该如何处理这些信息——我们已经开始在不同的上下文中接受不同的答案。

在日常推理中，我们做出所谓的开放世界假设：我们假定我们不了解一切，因此无法从缺乏知识中得出结论。也就是说，任何句子都可能是真的、假的或未知的。

```
                                OWA +             |  OWA +
                                Monotonic logic   |  Non-monotonic logic
Can derive P(true)      |   Can assert P(true)    |  Cannot assert P(true)
Can derive P(false)     |   Can assert P(false)   |  Cannot assert P(true)
Cannot derive P(true)   |   Unknown               |  Unknown
or P(false)
```

当我们做出开放世界的假设时，我们只能安全地断言一些我们可以从已知事物中推断出来的东西。我们关于世界的信息被认为是不完整的。

我们先来看一个例子，我们知道我们的推理是单调的。在这种情况下，我们所拥有的任何（可能不完整的）知识都不能通过学习新知识而失效。因此，如果我们根据一些推论推断出一个句子是正确的，比如“含有两大汤匙番茄酱的东西是蔬菜”和“比萨含有两大汤匙番茄酱”，那么我们就可以得出“比萨是蔬菜”的结论。如果我们能推断出一个句子是假的，情况也是一样的。

然而，如果我们不能推断出任何东西——例如，我们掌握的一套知识包含顾客信息，而没有关于比萨饼或蔬菜的信息——那么在开放世界的假设下，我们必须说我们不能得出任何结论。

有了非单调的知识，我们现在知道的任何东西都有可能失效。因此，我们不能安全地得出任何结论，即使我们能从我们目前所知道的推断出正确或错误。

然而，在数据库环境和许多计算机科学应用中，我们更倾向于作出更明确的结论。这意味着假设所谓的封闭世界假设：任何不能被证明是真的东西都是假的。这意味着不需要明确的虚假声明。换句话说，我们假设的事实数据库是完整的（最小的），所以任何不在其中的东西都可以被假定为是错误的。

例如，在CWA下，如果我们的数据库没有旧金山和赫尔辛基之间的航班的入口，那么我们可以安全地得出这样的结论：没有这样的航班存在。

我们还需要一件事来做出明确的断言：逻辑限定。限定是一种形式化的推测规则。域限定推测已知实体都存在。为了得出一个明确的结论，我们需要能够假设已知的实体都存在。

```
                                CWA +             |  CWA +
                                Circumscription + |  Circumscription +
                                Monotonic logic   |  Non-monotonic logic
Can derive P(true)      |   Can assert P(true)    |  Can assert P(true)
Can derive P(false)     |   Can assert P(false)   |  Can assert P(false)
Cannot derive P(true)   |   Can assert P(false)   |  Can assert P(false)
or P(false)
```

特别是，非单调推理需要这种假设。只有假设我们拥有完整的信息，我们才能做出自信的断言，因为其他信息可能会使我们的断言无效。

这在实践中意味着什么？首先，单调逻辑可以得出一个句子是真（或假）的结论。第二，非单调逻辑需要一个额外的假设：已知的实体都存在。

那么，为什么表面上的两种操作是等效的呢？为什么两个数相加是单调的，而计算两个节点上的聚合不是单调的？因为聚合不仅计算一个和，而且断言它已经看到了所有的值。唯一保证的方法是跨节点协调，并确保执行计算的节点确实看到了系统中的所有值。

因此，为了处理非单调性，我们需要使用分布式协调来确保断言只在所有信息都已知之后才进行，或者使用警告声明稍后结论可能无效的断言进行断言。

处理非单调性对于表现力（expressiveness）的原因很重要。这可以归结为能够表示非单调的事物；例如，能够说某列的总和是x是很好的。系统必须检测到这种计算需要一个全局协调边界，以确保我们已经看到了所有的实体。

纯单调系统是罕见的。似乎大多数应用程序都是在封闭世界的假设下运行的，即使它们有不完整的数据，我们人类也可以接受。当数据库告诉你在旧金山和赫尔辛基之间不存在直飞时，你可能会把这当作“根据这个数据库，没有直飞”，但是你不排除在现实中这样的飞行可能仍然存在的可能性。

实际上，这个问题只有在复制副本可能出现分歧时才会变得有趣（例如，在分区期间或由于正常操作期间的延迟）。然后需要更具体的考虑：答案是基于当前节点，还是基于系统的整体性。

此外，由于非单调性是由断言引起的，因此许多计算可以进行很长时间，并且仅在某些结果或断言传递给第三方系统或最终用户时应用协调似乎是合理的。当然，如果系统中的每个读写操作只是长时间运行计算的一部分，那么就不必强制执行总顺序。

## The Bloom language

The [Bloom language](http://www.bloom-lang.net/) is a language designed to make use of the CALM theorem. It is a Ruby DSL which has its formal basis in a temporal logic programming language called Dedalus.

在Bloom中，每个节点都有一个由集合和格组成的数据库。程序表示为与集合（事实集）和格（CRDT）交互的无序语句集。默认情况下，语句是顺序无关的，但也可以编写非单调函数。

Have a look at the [Bloom website](http://www.bloom-lang.net/) and [tutorials](https://github.com/bloom-lang/bud/tree/master/docs) to learn more about Bloom.

## Further reading

#### The CALM theorem, confluence analysis and Bloom

[Joe Hellerstein's talk @RICON 2012](http://vimeo.com/53904989) is a good introduction to the topic, as is [Neil Conway's talk @Basho](http://vimeo.com/45111940). For Bloom in particular, see [Peter Alvaro's talk@Microsoft](http://channel9.msdn.com/Events/Lang-NEXT/Lang-NEXT-2012/Bloom-Disorderly-Programming-for-a-Distributed-World).

- [The Declarative Imperative: Experiences and Conjectures in Distributed Logic](http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-90.pdf) - Hellerstein, 2010
- [Consistency Analysis in Bloom: a CALM and Collected Approach](http://db.cs.berkeley.edu/papers/cidr11-bloom.pdf) - Alvaro et al., 2011
- [Logic and Lattices for Distributed Programming](http://db.cs.berkeley.edu/papers/UCB-lattice-tr.pdf) - Conway et al., 2012
- [Dedalus: Datalog in Time and Space](http://db.cs.berkeley.edu/papers/datalog2011-dedalus.pdf) - Alvaro et al., 2011

#### CRDTs

[Marc Shapiro's talk @ Microsoft](http://research.microsoft.com/apps/video/dl.aspx?id=153540) is a good starting point for understanding CRDT's.

- [CRDTs: Consistency Without Concurrency Control](http://hal.archives-ouvertes.fr/docs/00/39/79/81/PDF/RR-6956.pdf) - Letitia et al., 2009
- [A comprehensive study of Convergent and Commutative Replicated Data Types](http://hal.inria.fr/docs/00/55/55/88/PDF/techreport.pdf), Shapiro et al., 2011
- [An Optimized conflict-free Replicated Set](http://arxiv.org/pdf/1210.3368v1.pdf) - Bieniusa et al., 2012

#### Dynamo; PBS; optimistic replication

- [Dynamo: Amazon’s Highly Available Key-value Store](http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf) - DeCandia et al., 2007
- [PNUTS: Yahoo!'s Hosted Data Serving Platform](http://scholar.google.com/scholar?q=PNUTS:+Yahoo!%27s+Hosted+Data+Serving+Platform) - Cooper et al., 2008
- [The Bayou Architecture: Support for Data Sharing among Mobile Users](http://scholar.google.com/scholar?q=The+Bayou+Architecture%3A+Support+for+Data+Sharing+among+Mobile+Users) - Demers et al. 1994
- [Probabilistically Bound Staleness for Practical Partial Quorums](http://pbs.cs.berkeley.edu/pbs-vldb2012.pdf) - Bailis et al., 2012
- [Eventual Consistency Today: Limitations, Extensions, and Beyond](https://queue.acm.org/detail.cfm?id=2462076) - Bailis & Ghodsi, 2013
- [Optimistic replication](http://www.ysaito.com/survey.pdf) - Saito & Shapiro, 2005