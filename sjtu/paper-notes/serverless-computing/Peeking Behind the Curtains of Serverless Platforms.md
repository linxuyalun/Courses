# Peeking Behind the Curtains of Serverless Platforms

* [Peeking Behind the Curtains of Serverless Platforms](https://www.usenix.org/system/files/conference/atc18/atc18-wang-liang.pdf)

serverless 是这几年变得火热的一个概念，强调FaaS。这允许租户专注于开发他们的函数——专用于特定任务的小型应用程序。 一个函数通常在具有受限资源（如CPU时间和内存，AWS Lambda根据CPU内存分了好几个套餐）的专用函数实例（容器或其他类型的沙箱）中执行。 与传统的IaaS平台中的VM不同，只有在调用函数时才会启动函数实例，并在处理请求后立即进入休眠状态。 租户按每次调用收费，无需支付未使用和闲置的资源，有趣的是，AWS Lambda给每个用户每个月提供了相当长的免费使用时间，可见其算力过剩。在最开始搜索serverless这个概念的时候，看到了一篇对于serverless讲的比较清晰的内容，[Serverless 应用开发指南](https://serverless.ink/)。

回到这篇文章，前面已经说了使用无服务器服务的一个好处是租户不支付函数实例空闲时消耗的资源。 租户仅在执行期间根据资源消耗进行计费。跨平台的共同点是对所有调用的聚合函数执行时间收费。 此外，价格取决于预配置的函数内存（AWS，Google）或调用期间（Azure）的实际消耗内存。 谷歌进一步根据CPU速度收取不同的费率。

这篇文章说serverless computing provider对于提供API的背后，是不透明的。这就会导致租户有理由对你提供的服务表示担忧：比如平台说不同函数实例之前是隔离的，那么，隔离的质量如何呢？比如说，你们这个服务器在受到DDoS等攻击时展现的安全性怎么样？你们程序性能如何？

于是他们对AWS Lambda，Azure Functions和Google Cloud Functions这三个大厂进行了大规模的测试，测试和测试！通过在这三个服务中启动了50,000多个函数实例，以表征其架构，性能和资源管理效率。他们的结果充当了流行的serverless平台的资源管理机制和效率的快照，为开发人员提供了构建更可靠平台的性能基准和设计选项，并帮助租户改进无服务器平台的使用。

## 测试方法

论文采用serverless用户的观点角度来描述serverless平台的体系结构，性能和资源管理效率。 论文在同一个云提供商区域设置有利位置，以通过官方API管理和调用来自一个或多个帐户的函数，并利用函数可用信息来确定重要特征。 论文通过调整函数配置和工作负载在各种设置下重复相同的实验，以确定可能影响测量结果的关键因素。

论文将所有必要的函数和子程序集成到一个我们称之为测量函数的函数（以各种语言实现了测量函数，但大多数为Python 2.7和Nodejs 6. *）中。 测量函数执行两个任务：

* 收集调用定时和函数示例运行时的信息，
* 基于接收的消息运行指定的子程序（例如，测量本地磁盘I / O吞吐量，网络吞吐量）。 

测量函数通过Linux上的`proc`文件系统（`procfs`），环境变量和系统命令收集运行时信息。 它还报告执行开始和结束时间，调用ID（由唯一标识调用的函数生成的随机16字节ASCII字符串）和函数配置以便于进一步分析。

## serverless架构揭秘

论文利用阅读官方文档和测量函数尝试逆向设计AWS， Azure和Google的体系结构。为什么论文要这么做？因为如果我们能够知道它们的体系结构，那么它有助于减少实验中的噪音并获得更准确的结果。

先看一下总的测试实验结论。

**AWS**。 函数在专用函数实例中执行。 从AWS上面还是可以获取许多想要的东西的，测量结果表明，函数的不同版本将被视为不同的并在不同的函数实例中执行。 `procfs`文件系统公开底层VM主机的全局统计信息，而不仅仅是函数实例，并包含用于分析运行时，识别主机VM等的有用信息。 从`procfs`，论文发现主机VM主要有2个vCPU和3.75 GB物理RAM。

**Azure。** Azure Functions使用Function Apps来组织函数。 对应于一个函数实例的function app是包含各个函数的执行环境的容器。 函数实例中的环境变量包含有关主机VM的一些全局信息。 收集的环境变量表明主机VM可以具有1,2或4个vCPU。

**Google。**Google隔离并过滤可从`procfs`访问的信息，因此论文无法获得有关运行时的大量信息。 

以下分别展开。

识别serverless的**VM**是一个十分重要的内容。在AWS中， `/proc/self/cgroup`文件有一个特殊的条目，称之为实例根ID，实验发现共享相同实例根ID的实例位于同一个VM上，论文通过了一种基于I/O的coresidency（共存） 测试来进行实验，测试方法可以看论文。我们调用通过从实例VM公共IP查询IP地址查找工具获得的IP，以及从运行`uname`命令VM私有IP获得的IP。 共享相同实例根ID的函数实例具有相同的VM公共IP和VM专用IP。在Azure中， 根据官方文件[6]，WEBSITE_INSTANCE_ID环境变量用作VM标识符，将其称为Azure VM ID。 论文通过共享的DLL使用Flush-Reload来验证共享相同Azure VM ID的实例的coresidency， 结果表明Azure VM ID是一个强大的VM标识符（整个关于Azure的这段话没看懂什么意思）。在Google中，论文找不到任何可以让我们主机的信息。 使用基于I / O的coresidency不起作用，因为正如之前所说的，Google隔离并过滤`procfs`，`procfs`不包含全局使用统计信息。 

之前的研究表明，AWS EC2中的coresidency导致虚拟机会被攻击。 通过对实例与VM关系的认知，论文检查了租户的主要资源——函数实例，是如何被**隔离**的。 论文假设一个租户对应一个用户帐户，并且只考虑VM级别的coresidency。

在AWS上， 无论配置和代码如何，同一租户创建的函数都将共享同一组VM。AWS为每个租户分配不同的VM。 **测试方法**：在每一轮中，我们同时在两个帐户的每个帐户下创建一个新函数，在一个函数中写入一个随机数字的字节，并检查另一个函数中的磁盘使用情况统计信息。我们运行了这个测试1周，但没有发现跨租户函数实例的VM-coresidency。

在Azure上，Azure Functions是Azure App服务的一部分，在Azure，所有租户共享同一组VM。 因此，Azure Functions中的租户也应共享VM资源。**测试方法**：在两个帐户中分别调用了500个函数，发现30％的函数实例与其他帐户中的函数实例共存，共执行120个虚拟机。 不过截至到2018年5月，不同的租户不再共享Azure中的相同VM。

论文发现所有考虑过的服务中的VM都有各种配置，这可能由基础架构升级导致的变化可能导致函数性能不一致。比如在AWS中，发现了五种类型的VM。Azure显示更多的VM配置。 Azure中使用了九种（至少）不同类型的VM。 性能可能会根据运行该函数的主机类型（更具体地说，vCPU的数量）而有很大差异。在Google中，型号名称始终为“未知”，但有4种独特的型号版本。

所有这些serverless供应商都不会完全隐藏租户的运行时信息，所以更多关于实例运行时和后端基础架构的知识可以使攻击者更容易在函数实例中查找漏洞。 比如，论文已经通过`procfs`获取了大量信息，实际上可以使用它来监视coresidency实例的活动。虽然看似无害，但专门的对手可能会将其作为更复杂攻击的基石。因此除非必要，否则应限制对运行时信息的访问以用于安全目的。 另外，提供商应以可审计的方式公开此类信息，即通过API调用，以便他们能够检测并阻止可疑行为。

## 资源调度

论文研究了如何在实例**冷启动延迟，生命周期，可伸缩性**等方面在三个无服务平台中调度实例和VM。

首先是**可伸缩性和实例放置**，正如这些厂商BB的那样，响应于需求变化的弹性自动缩放是serverless模型的益处。因此需要衡量平台扩展的程度。

**测试方法**：创建了40个相同内存大小f1，f2，...，f40的测量函数，对于每个fi，用5i个并发请求调用它。 我们在批量调用之间暂停了10秒，以应对平台中的速率限制。 所有测量函数只需睡眠15秒然后返回。 对于每种配置，进行了50轮测量。

在支持并发执行方面，AWS是三项服务中最好的。N个并发调用总是产生N个并发运行的函数实例。 AWS可以轻松扩展到200个（最大测量并发级别）新鲜函数实例。在实例放置方面，AWS Lambda似乎将它为bin-packing问题，采取了一种贪心策略，尝试在现有活动VM上放置新的函数实例以最大化VM内存利用率，即实例内存大小之和除以3,328（这是测量到的最大VM聚合内存）。

Azure文档声明它将自动扩展到最多200个实例，用于基于Nodejs的单个函数，并且每10秒最多可以启动一个新的函数实例。 但是，在测试中，无论如何更改调用之间的间隔，最多只能看到10个函数实例同时为单个函数运行。 所有请求都由一小组函数实例处理。 并发运行的实例都不在同一个VM上。 因此，似乎Azure不会尝试在同一个VM上共同定位相同函数的函数实例

Google未能提供所需的可扩展性，即使Google声称HTTP触发的函数会迅速扩展到所需的调用率。 通常，即使对于低并发级别（例如，10），也只有大约一半的预期实例数量可以同时启动，而其余的请求是排队的。

然后是**冷启动和VM配置**。使用冷启动来表示启动新函数实例的过程。 对于平台，冷启动可能涉及启动新容器，设置运行时环境以及部署函数，与重用现有函数实例（热启动）相比，这将花费更多时间来处理请求。 因此，冷启动可以显着影响应用程序响应性，进而影响用户体验。

**测试方法**：对于每个平台，我们创建了1,000个相同内存和语言的不同函数，并依次调用它们中的每一个以收集其冷启动和热启动延迟。 我们使用调用发送时间（由有利位置记录）和函数执行开始时间（由函数记录）的差异作为其冷启动/热启动延迟的估计。 作为基线，AWS，Google和Azure的热启动延迟中位数在所有调用中分别约为25,79和320毫秒。

实验结果还是比较有意思的。在AWS中，检查了两种类型的冷启动事件：在（1）以前从未见过的新VM上启动，以及（2）在现有VM上启动函数实例。 直观地，情况（1）应该具有比（2）明显更长的冷启动延迟，因为情况（1）可能涉及启动新VM。 但是，发现案例（1）一般只略长于（2）。另外，发现的最小VM内核正常运行时间（来自`/proc/uptime`）为132秒，表明VM已在调用之前启动。因此，AWS拥有一个准备好的VM池。 情况（1）中的额外延迟更可能通过调度（例如，选择VM）而不是启动VM来引入。

函数内存和语言影响冷启动延迟，比如Python 2.7实现了最低中位冷启动延迟（167-171 ms），而Java函数的延迟明显高于其他语言（ 824-974 ms）。 冷启动延迟通常随着函数内存的增加而降低。 一种可能的解释是AWS会根据内存大小按比例分配CPU功率; 随着CPU功率的增加，环境设置变得更快。

由于AWS的实例放置策略，可以在同一VM上同时启动许多函数实例。 在这种情况下，随着更多实例同时启动，冷启动延迟会增加。 比如，在给定VM上启动具有128 MB内存的基于Python 2.7的函数的20个函数实例平均花费1,321 ms，这比在同一VM上启动1个函数实例（186 ms）慢约7倍。

Azure和Google。Google也会按比例分配内存，但Google内存大小对冷启动延迟的影响要大于AWS。 在Azure中启动函数实例需要更长的时间，尽管它们的实例总是分配1.5 GB的内存。 Azure中的冷启动延迟中位数为3,640毫秒。 漫长的延迟是由Azure所知并正在努力改进的平台中的设计和工程问题引起的。

AWS和Google都在秀，只有Azure在挨揍= =

在2018年5月重复了的冷启动测量。没有发现AWS中冷启动延迟的重大变化， 但是谷歌的冷启动延迟时间平均缩短了4倍，这可能是由于2018年2月的基础设施更新，Azure缩短了15倍。 所以开发一个serverless测量平台还是很重要的。

关于**实例生命周期**。将函数实例保持活动的最长时间定义为实例生命周期。 租户当然更喜欢长寿命，因为他们的应用程序将能够更长时间地保持内存状态（例如，数据库连接）并且从冷启动中受到更少的影响。

**测试方法**：为了估计实例生命周期，设置了不同内存大小和语言的函数，并以不同的频率调用它们（每5/30/60秒一个请求）。 函数实例的生命周期是第一次和最后一次看到实例之间的时间差异。 将实验运行了7天（AWS和Google）或更长时间（Azure），以便我们可以在给定设置下收集至少50个生命周期。

Azure函数实例的生命周期显着长于AWS和Google。在AWS中，所有设置的实例生命周期中值为6.2小时，最大值为8.3小时。 AWS中的主机VM通常寿命更长：观察到的最长VM内核正常运行时间为9.2小时。 当请求频率增加时，实例寿命反而趋于变短。 谷歌总体生命周期偏短，更大的内存往往具有更长的生命周期。 例如，当每五秒调用一次时，Google的128MB和2,048 MB内存的90％实例的生命周期分别为3-31分钟和19-580分钟。 因此，对于在繁重工作负载下具有小内存的函数，Google似乎积极地推出新实例而不是重用现有实例。 这可能会增加冷启动的性能损失.

关于**闲置实例回收**。为了有效地使用资源，serverless提供商关闭空闲实例以回收分配的资源。 我们定义实例在关闭之前可以保持空闲的最长时间，作为实例最大空闲时间。 *在更长的生命周期和更短的空闲时间之间存在权衡，因为维护更多空闲实例是浪费VM内存资源，而更少的准备服务实例导致更多冷启动。*

**测试方法：**对导致不同函数实例的函数的两次调用之间的最小延迟t<sub>idle</sub>执行了二元搜索。 首先创建了一个函数，调用它两次，延迟时间在1到120分钟之间，并确定这两个请求是否使用了相同的函数实例。 重复，直到确定了t<sub>idle</sub>。 通过对接近t<sub>idle</sub>的延迟重复测量100次来确认t<sub>idle</sub>（以微小的粒度）。

在AWS中， 一个实例通常可以保持不活动状态最多27分钟。 如果VM上存在活动实例，则实例可以保持非活动状态较长时间。 通过每10秒发送一个请求，在给定的VM上保持一个实例处于活动状态，并发现：AWS仍然采用相同的策略来回收相同函数的空闲实例，但是观察到一些空闲实例在这种情况下可能会闲置1-3个小时。

Azure和Google。 在Azure中，我们找不到一致的最大实例空闲时间。 我们在不同的日子重复了几次实验，发现最大空闲时间分别为22,40和120分钟。 在Google中，实例的空闲时间可能超过120分钟。 120分钟后，实例在18％的实验中保持活跃状态。

最后是一些**不一致的函数使用**。租户期望函数更新后的请求应该由新的函数代码处理，特别是如果更新是安全关键的。但是，论文在AWS中发现（尽管概率很小）旧版本的函数可以处理请求的。称这种情况不一致的函数使用。在实验中，向函数发送了k = 1或k = 50个并发请求，并在更新函数的以下方面之一后再次**无延迟**地执行此操作：IAM角色，内存大小，环境变量或函数代码。对于给定的设置，执行了100轮的这些操作。查看所有轮次的所有测试，我们发现3.8％的实例运行不一致的函数。通过检查案例，发现了两种情况：（1）AWS启动了过时函数的新实例（占所有案例的2％），以及（2）AWS重用过时函数的现有实例。不一致的实例在终止之前永远不会处理多个请求（请注意，AWS中的最大执行时间为300秒），但是，相当多的请求可能无法获得所需的结果。而如果在函数更新后等待较长时间来发送请求，则发现较少的不一致情况，并且最终为0，等待时间为6秒。 因此，不一致问题应当是由实例调度程序中的竞争条件引起的。 这说明协调多个函数实例之间的函数更新具有挑战性，因为调度程序无法进行原子更新。

## 性能隔离

性能隔离主要分为两方面，一方面是CPU利用率，一方面是I/O和网络。

为了测量**CPU利用率**，**测试方法**是测量函数使用`time.time()`（Python）或`Date.now()`（Nodejs）连续记录时间戳1,000 ms。度量实例CPU利用率定义为记录时间戳的1,000毫秒的分数，即在1,000毫秒的时间里使用了多少时间的CPU。

根据AWS，函数实例的CPU功率与其预先配置的内存成比例。 但是，AWS没有详细说明如何为实例分配CPU时间。论文进一步研究了如何在共存实例之间分配CPU时间，检查结果发现运行的实例公平地共享CPU，因为它们具有几乎相同的CPU利用率。 因此，AWS尝试仅基于函数内存为实例分配固定数量的CPU周期。

Azure和Google。 Google采用与AWS相同的机制来根据函数内存分配CPU周期。 在Google中，随着函数内存的增加，实例CPU利用率的中位数从11.1％到100％不等。 对于给定的memory大小，不同实例的速率的标准偏差非常低。Azure的CPU利用率差异较大，即使为实例分配了相同数量的内存，也是如此。 4-vCPU虚拟机上的实例往往获得更高的CPU份额。 1-vCPU VM和2-vCPU VM上实例的利用率分布实际上是相似的; 但是，当共存函数实例增加时，1-vCPU VM上的实例的CPU利用率下降得更厉害。

关于**I / O吞吐量**和**网络吞吐量**，在AWS中， 虽然聚合I / O和网络吞吐量保持相对稳定，但随着共存实例的增加，每个实例在I / O和网络资源中的份额都会减少。  在Azure中，实例的I / O和网络吞吐量也随着`colevel`的增加而下降，并且由于来自其他coreident实例的争用而波动。 更有趣的是，资源分配是根据函数实例恰好安排在哪种类型的VM来区分的。 在Google中，随着函数内存的增加，测量的I / O和网络吞吐量都会增加。但是Google在具有相同内存大小的不同实例测量的网络吞吐量会发生显着变化。 例如，在2,048 MB函数实例中测量的网络吞吐量在0.2 Mbps和321.4 Mbps之间波动。 论文发现了两种情况：（1）所有实例吞吐量在给定时间段内波动，与内存大小无关，或者（2）单个实例暂时遭受吞吐量下降的影响。 情况（1）可能是由于网络状况的变化，而情况（2）导致怀疑GCF租户实际上共享主机并遭受资源争用。更具体的测量方法和测试结果可以看论文详细内容。

以上的测试结果说明，AWS和Azure无法在共存实例之间提供适当的性能隔离，**可以说这方面有些细节处理的很差**，因此争用可能会导致性能显着下降。 在AWS中，他们将来自同一帐户的函数实例打包到VM上的事实意味着扩展函数会将相同的函数放在同一个VM上，从而导致资源争用和延长执行时间（更不用说更长的冷启动延迟）。 Azure具有类似的问题，另外一个问题是VM之间的争用在帐户之间产生。 后者还为服务攻击的交叉租户降级提供了可能性。

总的来说，这篇文章提供了三种现代无服务计算平台的体系结构，资源利用率和性能隔离效率的见解。 论文在平台的安全性，性能和资源会计方面发现了许多问题，这些问题来自特定的设计决策或工程。 论文的结果为未来无服务平台设计中提高资源利用率和隔离性的研究提供了机会。





[返回目录](../README.md)