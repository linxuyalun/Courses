# MapReduce

- [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf)

MapReduce是Google 在OSDI 2004年发表的一篇文章，在看完之后发现有一篇完整的[译文](https://www.aliyun.com/zixun/wenji/1284918.html)。

作者和许多其他人在MapReduce提出之前为了处理海量原始数据（raw data）时发现想要在合理的时间内完成计算只有在上千台机器上分布式并行处理。这带来了三个复杂的问题：如何处理并行计算？如何分发数据？如何处理故障？解决这些问题需要大量的代码处理。

MapReduce就是根据以上问题设计的一个抽象模型。使用MapReduce模型，开发人员只需要表示他们想要执行的操作，而不必关心并行计算、容错、 数据分布、负载均衡等复杂细节，这些问题都被封装在了MapReduce库。

MapReduce就是一个可以处理和生成超大数据集的编程模型。原理是利用一个输入键值对集合来产生一个输出的键值对集合。MapReduce库的用户用两个函数表达这个计算：`Map`和`Reduce`。`Map`函数用来接收一个键值对然后生成**一组**中间键值对，然后在所有具有相同中间key的中间value上应用`Reduce`操作，合并中间的数据，得到一个想要的结果。

文章举例了一些十分适合用MapReduce的模型，比如：

- 分布式Grep：`Map`函数输出匹配某个模式的一行,`Reduce`函数是一个恒等函数,把中间数据复制到输出；
- 计算URL访问频率：`Map`函数处理日志中web页面请求的记录,然后输出`(URL,1)`。`Reduce`函数把相同URL的value值都累加起来为`(URL,total)`；
- 反索引：`Map`函数分析每个文档输出一个`(word,Document ID)`的列表,`Reduce`函数的输入是一个给定词的所有`(word,Document ID)`，排序所有的文档ID，输出`(word,list(Document ID)`。所有的输出集合形成一个简单的反索引。

以下关于实现细节具体展开：

MapReduce接口的许多不同实现都是可能的。 正确的选择取决于环境。这一节讲了Google计算环境上的实现：通过交换式以太网连接在一起的大型商用PC集群。这个环境有一些特点，具体见论文

## 并行计算：

MapReduce要干这么一件事：`Map`调用的输入数据自动分割成M个数据片段的集合，`Map`调用被分布到多台机器上执行。输入的数据片段能够在不同的机器上并行处理。使用分区函数将`Map`调用产生的中间键分成R个不同分区（如,hash(key) mod R），`Reduce`调用也被分布到多台机器上执行。分区数量R和分区函数由用户来指定。

下图非常清晰地描述了MapReduce操作的总体流程，图中的编号标签对应于下面中的数字：

![论文图1](https://pic4.zhimg.com/80/f7fbb747fc3e7a42112f1ccf82cfa1c7_hd.jpg)



1. 用户程序中的MapReduce库首先将输入文件拆分为每件通常16MB到64MB的M个片段。 然后它在一组机器上启动程序的许多副本。
2. 其中一个程序的副本被任命为master，负责调度，相当于公司老板。 其余的是由master分配工作的worker，相当于干活的员工。 有M个`map` 任务和R 个`reduce` 任务要分配。 Master挑选闲置的worker并为他们分配一个`map` 任务或`reduce`  任务。（一个worker要么负责`map`，要么负责`reduce`）
3. 分配了`map`任务的worker将读取相应分割后的输入内容。 它从输入数据中解析键值对，并将每对传递给用户定义的`Map`函数。 `Map`函数生成的中间键值对在内存中缓冲。
4. 周期性地，缓冲的键值对被写入进本地磁盘，通过分区函数划分为R区域。 这些缓冲的键值对在本地磁盘上的位置将传递回master，master负责将这些位置转发给`reduce` worker。
5. 当master通知一个`reduce`  worker 这些位置时，它会从`map` worker的本地磁盘读取缓冲数据。 当`reduce` worker读取了所有中间数据时，它会通过中间键对其进行排序，以便将所有出现的相同键组合在一起。 因为通常许多不同的键映射到相同的`reduce` 任务，因此排序是需要的。
6. `reduce` worker遍历已排序的中间数据，并且对于遇到的每个唯一的中间键，它将键和相应的一组中间值集传递给用户的`Reduce`函数。 `Reduce`函数的输出附加到此`reduce`分区的最终输出文件。
7. 完成所有`map`任务和`reduce`任务后，master会唤醒用户程序。 此时，用户程序中的`MapReduce`调用返回用户代码。

成功完成后，MapReduce执行的输出在R个输出文件中可用（每个`reduce`任务一个，文件名由用户指定）。 通常，用户不需要将R个输出文件合并到一个文件中 ——他们经常将这些文件作为输入传递给另一个MapReduce调用，或者从另一个能够处理分区为多个文件的输入的分布式应用程序中使用它们。

master有几个数据结构。 它存储每个`map`任务和`reduce`任务的state（空闲，正在进行或已完成）以及worker 机器的id（用于非空闲任务）。

## 故障处理：

因为MapReduce库的设计初衷是使用由成百上千的机器组成的集群来处理超大规模的数据,所以,这个库必须要能很好的处理机器故障。故障可以分为worker故障和master故障。

处理worker故障的方法很简单。master定期 ping每个worker。如果在一定时间内未收到worker A的响应，则master将A标记为failed。A中任何已经完成的`map` 任务都将重置回其初始空闲状态，因此master可以在其他worker上调度。 同样，A上正在进行的任何`map`任务或`reduce`任务也会重置为空闲状态。这些完成的`map`任务在发生故障时重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问。完成的`reduce`任务不需要重新执行，因为它们的输出存储在全局文件系统中。

当一个`map`任务首先由worker A执行然后由worker B执行时（因为A失败），所有正在进行的`reduce`任务的worker都会收到重新执行的通知。 任何尚未从A读取数据的`reduce`任务将从B读取数据。

作者认为鉴于只有一个master，挂的可能性不大；万一挂了，当前的实现方法是中止计算。 客户端可以检查情况并根据需要重试MapReduce操作。

## 数据分布/局部性：

网络带宽是一种稀缺的资源。一直在说MapReduce需要处理海量的数据。那么数据在哪里？数据实际上是随机地存储于机器上的。我们不需要统一地把数据一起存到一个超大的硬盘上，数据可以直接散布在这些个PC上，这些PC自身不仅是许许多多个处理器，也是许许多多个小硬盘。 我们利用输入数据存储在组成我们集群的机器的本地磁盘上，从而节省网络带宽。 GFS将每个文件划分为64 MB块，并将每个块的多个副本（使用副本保证数据的有效性，通常为3个副本）存储在不同的计算机上。

MapReduce采取了一种局部性的策略：master会考虑输入文件的位置信息，并尝试在包含相应输入数据副本的计算机上计划`map`任务。 如果不这样做，它会尝试在该任务的输入数据的副本附近安排`map`任务（例如，在与包含数据的机器位于同一网络交换机上的机器上）。这样，大多数输入数据在本地读取并且不消耗网络带宽。

## 备份任务：

延长MapReduce操作总时间的常见原因之一是“落后者”：一台机器需要花费非常长的时间来完成计算中最后几个`map`或`reduce`任务之一。 解决的机制是当MapReduce操作接近完成时，master会备份正在进行的任务，然后调度备份任务。 无论是原执行任务还是备份执行任务完成，任务都会标记为已完成。 这种机制通常会将操作使用的计算资源增加不超过百分之几，但大大减少了完成大型MapReduce操作的时间。 

## 几个有意思的优化：

- Combiner Function：在某些情况下，每个`map`任务产生的中间键都会有重大的重复，而用户指定的`Reduce`函数是可交换和关联的。 比如单词计数示例，因此每个`map`任务将产生数百或数千个形式为<the，1>的记录。 所有这些计数将通过网络发送到单个`reduce`任务，然后通过`Reduce`函数一起添加以生成一个数字。 MapReduce允许用户指定一个可选的`Combiner`函数，该函数在通过网络发送之前对这些数据进行部分合并。`Combiner` 函数在执行`map`任务的每台机器上执行。 通常，相同的代码用于实现`Combiner`和`reduce`函数。 `reduce`函数和`Combiner` 函数之间的唯一区别是MapReduce库如何处理函数的输出。 `reduce`函数的输出被写入最终输出文件。 `Combiner` 的输出被写入将发送到`reduce`任务的中间文件。
- Skipping Bad Records：有时用户代码中存在bug导致`Map`或`Reduce`崩溃。  通常的做法是修复错误，但有时这是不可行的；也许这个bug存在于第三方库中，源代码不可用。 因此，忽略一些记录是可以接受的，例如在对大型数据集进行统计分析时，提供了一种可选的执行模式，其中MapReduce库检测哪些记录导致确定性崩溃并跳过这些记录以便前进。
- Local Execution：调试`Map`和`Reduce`函数的bug是非常困难的，因为实际执行操作时不但是分布在系统中执行的，而且通常是在好几千台计算机上执行，具体的执行位置是由master进行动态调度的。为了简化调试、profile和小规模测试，开发了一套MapReduce库的本地实现版本，MapReduce操作在本地计算机上顺序的执行。用户可以控制MapReduce操作的执行，可以把操作限制到特定的`Map`任务上。用户通过设定特别的标志来在本地执行他们的程序，之后就可以很容易的使用本地调试和测试工具。
- Status Information：master使用嵌入式的HTTP服务器显示一组状态信息页面,用户可以监控各种执行状态。状态信息页面显示了包括计算执行的进度,比如已经完成了多少任务、有多少任务正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理百分比等等。页面还包含了指向每个任务的 stderr和stdout文件的链接。用户根据这些数据预测计算需要执行大约多长时间、是否需要增加额外的计算资源。这些页面也可以用来分析什么时候计算执行的比预期的要慢。另外,处于最顶层的状态页面显示了哪些worker失效了,以及他们失效的时候正在运行的Map和Reduce任务。这些信息对于调试用户代码中的bug很有帮助。

## 性能衡量

论文用一个大型集群上运行的两个计算来衡量MapReduce的性能。**一个计算在大约1TB的数据中进行特定的模式匹配,另一个计算对大约1TB的数据进行排序**。选择这两个程序在大量的使用MapReduce的实际应用中是非常典型的——一类是对数据格式进行转换，从一种表现形式转换为另外一种表现形式；另一类是从海量数据中抽取少部分的用户感兴趣的数据。具体的实验结果不再赘述。

到04年为止,MapReduce最成功的应用就是重写了Google网络搜索服务所使用到的**索引系统**。索引系统的输入数据是网络爬虫抓取回来的海量的文档，这些文档原始内容的大小超过了20TB。索引程序是通过一系列的MapReduce操作(大约5到10次)来建立索引。使用MapReduce带来这些好处:

- 实现索引部分的代码简单、小巧、容易理解，因为对于容错、分布式以及并行计算的处理都是MapReduce库提供的。比如，代码行数从原来的3800行C++代码减少到大概700行代码；
- MapReduce库的性能已经足够好了，因此我们可以把在概念上不相关的计算步骤分开处理，而不是混在一起以期减少数据传递的额外消耗。概念上不相关的计算步骤的隔离也使得我们可以很容易改变索引处理方式；
- 索引系统的操作管理更容易了。因为由机器失效、机器处理速度缓慢、以及网络的瞬间阻塞等引起的绝大部分问题都已经由MapReduce库解决了，不再需要操作人员的介入了。另外，我们可以通过在索引系统集群中增加机器的简单方法提高整体处理性能。



[返回目录](../README.md)